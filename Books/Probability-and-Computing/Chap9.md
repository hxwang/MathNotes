## Entropy, Randomness and Information

- reading status: ing 10/09/2014


### Contents
- Definition of entropy
- Sum of entropy
    <div style="text-align:center" markdown="1">
        <img src="./figs/chap9_entropySum.PNG" width="700px" />
    </div>
- Entropy and binomial
    <div style="text-align:center" markdown="1">
        <img src="./figs/chap9_entropyAndBino.PNG" width="550px" />
    </div>
    - interpretation: if we flip a biased coin with p =3/4 to get head, then we obtain H(3/4) random bits each time we flip this coin 
    - the bits to represent the possible type of outputs
- Interpreting entropy
    - e.g., compression
- Shannon's Theorem
    - coding theory studies the trade-off between the amount of redundancy required and the probability of decoding error over various types of channel

### Questions
