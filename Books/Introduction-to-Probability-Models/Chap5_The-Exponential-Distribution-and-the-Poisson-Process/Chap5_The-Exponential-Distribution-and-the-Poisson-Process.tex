%\documentclass[10pt,conference]{IEEEtran}

\documentclass[10 pt,final]{article}

\usepackage{amssymb} \usepackage{amsmath} \usepackage{amsthm} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{url} \usepackage[margin=1in]{geometry}

\usepackage{subfigure}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}{Lemma} \newtheorem{proposition}{Proposition} \newtheorem{corollary}{Corollary} \newtheorem{definition}{Definition} \newtheorem{assumption}{Assumption} \newtheorem{example}{Example}
\newtheorem{observation}[theorem]{Observation}
%\newtheorem{theorem}{Theorem} \newtheorem{definition}{Definition} \newtheorem{remark}{Remark} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{fact}{Fact} \newtheorem{invariant}{Invariant}

\usepackage{color}
\newcounter{todocounter}
\newcommand{\todo}[1]{\stepcounter{todocounter}\textcolor{red}{to-do\#\arabic{todocounter}: #1}}
\newcommand{\impo}[1]{{\color{magenta} #1}}
\newcommand{\question}[1]{{\color{blue} #1}}



\usepackage{graphicx}
\graphicspath{{./Figures/}}

\title{Chap 5 Note: The Exponential Distribution and the Poisson Process}


\begin{document}



%\author{Huangxin Wang\thanks{Department of Computer Science, George Mason University. Fairfax, VA 22030. Email: \textsf{hwang14@gmu.edu}}}
\date{}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Exponential Distribution} 
\subsection{Definition}

A continuous random variable $X$ is said to have an exponential distribution with parameter $\lambda, \lambda > 0$, if its probability density function is given by
\begin{align*}
f(x) =
\begin{cases}
\lambda e^{-\lambda x} & x \geq 0\\
0 & x <0
\end{cases}
\end{align*}

or, equivalently, if its cdf is given by
\begin{align*}
F(x) = \int_{-\infty}^{x} f(y)dy = 
\begin{cases}
1 -e^{-\lambda x} & x \geq 0 \\
0 & x <0
\end{cases}
\end{align*}

Also, we have \impo{$P\{x>\beta\} = e^{-\lambda \beta}$}.

Check page 278-279 for the calculation of moment, mean, variance

\paragraph{Example} (Exponential Random Variables and Expected Discounted Returns) Suppose that you are receiving rewards at randomly changing rates continuously throughout time. Let $R(x)$ denote the random rate at which you are receiving rewards at time $x$. For a value $\alpha \geq 0$, called the discount rate, the quantity 
\begin{align*}
R = \int_0^{\infty} e^{-\alpha x} R(x) dx
\end{align*}

represents the total discounted reward.  Then we have
\begin{align*}
\int_0^{\infty} e^{-\alpha x} E[R(x)] = E[\int_0^T R(x)dx]
\end{align*}

where $T$ is an exponential random variable with rate $\alpha$. Check page 279 for the proof.

Thus, \impo{the expected total discounted reward is equal to the expected total(undiscounted) reward eared by a random time that is exponentially distributed with rate equal to the discount factor.}


\subsection{Properties of the Exponential Distribution}
\paragraph{MemoryLess} A random variable $X$ is said to be without of memory, or memoryless, if
\begin{align*}
P\{X > s+t| X>t\} = P\{X>s\} \mbox{  for all $s,t \geq 0$}
\end{align*}

The condition of above equation is equivalent to
\begin{align*}
\frac{P\{X>s+t, X>t\}}{P\{X>t\}} = P\{X>s\}
\end{align*}

The following equation is satisfied if $X$ is exponentially distributed. Thus, exponentially distributed random variables are memoryless.

\impo{
\begin{align*}
P\{X>s+t\} = P\{X>s\} * P\{X>t\}
\end{align*}
}

\paragraph{Example} Consider a post office that is run by two clerks. Suppos that when Mr. Smith enters the system he discovers that Mr. Jones is being served by one of the clerks and Mr. Brown by the other. Suppose also that Mr. Smith is told that his service will begin as soon as either Jones or Brown leaves. If the amount of time that a clerk spends with customer is exponentially distributed with mean $1/\lambda$, what is the probability that, of the three customers, Mr. Smith is the last to leave the post office?

\textbf{Solution:} Consider the time at which Mr. Smith first finds a free clerk. At this point either Mr. Jones or Mr. Brown would have just left and the other one would still be in service. \impo{However, by the lack of memory of the exponential, it follows that the amount of time that this other man(either Jones or Brown)} would still have to spend in the post office is exponentially distributed with mean $1/\lambda$. That is, \impo{it is the same as if he were just starting his service at this point}. Hence, by symmetry, the probability that he finishes before Smith must equal $\frac{1}{2}$.

\paragraph{Example(Mean and Variance of New Variable)} The dollar amount of damage involved in an automobile accident is an exponential random variable with mean $1000$. Of this, the insurance company only pays the amount exceeding(the deduction amount of) 400. Find the expected value and the standard deviation of the amount of the insurance company pays per accident.

\textbf{Solution:} If $X$ is the dollar amount of damage resulting from an accident, then the amount paid by the insurance company is $(x-400)^+$. Let
\begin{align*}
I =
\begin{cases}
1 & \mbox{ if $X> 400$} \\
0 & \mbox{ if $X \leq 400$}
\end{cases}
\end{align*}

Let $Y$ be the amount paid. By the lack of memory property of the exponential, then we have
\begin{align*}
& E[Y|I=1] = 1000 \\
& E[Y|I=0] = 0 \\
& Var(Y|I=1) = (1000)^2 \\
& Var(Y|I=0) = 0
\end{align*}

which can be conveniently written as
\impo{
\begin{align*}
& E[Y|I] = E[Y|I=1]*1 + E[Y|I=0]*0 = 10^3 I \\ 
& Var(Y|I) = 10^6 I
\end{align*}
}

Because I is a Bernoulli random variable that is equal to $1$ with probability \impo{$e^{-0.4}$}, we obtain
\begin{align*}
E[Y] = E[E[Y|I]] = 10^3*E[I] = 10^3*e^{-0.4}
\end{align*}

and, by the \impo{conditional variance formula}, we have
\begin{align*}
Var(Y) & = E[Var(Y|I)] + Var(E[Y|I]) \\
& = 10^6 * e^{-0.4} + 10^6 *(1-e^{-0.4})*e^{-0.4}
\end{align*}

Note we use the property that the variance of a Bernoulli random variable with parameter $p$ is $p(1-p)$.

\paragraph{Example} How much should be ordered so as to maximize the stores' expected profit.

\textbf{Comments:} check page $283$ for more details. The memoryless property being used is 
\begin{align*}
E[(X-t)^+] & = E[(X-t)^+ | X>t] P(X > t) + E[(X-t)^+ | X \leq t] P(X \leq t) \\
& = \impo{E[(X=t)^+ | X>t]} * e^{-\lambda t} \\
& = \impo{\frac{1}{\lambda}} * e^{-\lambda t} 
\end{align*}

\subsection{Failure (or hazard) rate function} Consider a continuous positive random variable $X$ having distribution function $F$ and density $f$. The \emph{failure (or hazard) function rate} is defined by
\begin{align*}
r(t) = \frac{f(t)}{1-F(t)}
\end{align*}

To interpret $r(t)$, suppose that an item, having lifetime $X$, has survived for $t$ hours, and we desire the probability that it does not survive for an additional time $dt$. That is, consider $P\{X \in (t, t+dt) | X>t\}$. Now
\begin{align*}
P\{X \in (t, t+dt)| X>t\} & = \frac{P\{X \in (t, t+dt) | X>t\}}{P\{X>t\}} \\
& = \frac{P\{X \in (t, t+dt)\}}{P\{X>t\}} \\
& \approx \frac{f(t)dt}{1-F(t)} \\
& r(t)dt
\end{align*}

\begin{align*}
r(t) = \frac{f(t)}{1-F(t)} = \lambda
\end{align*}

Thus the failure rate function for the exponential distribution is constant. The parameter $\lambda$ is often referred to as the rate of the distribution.

\paragraph{Example} Let $X_1, \cdots, X_n$ be independent exponential random variables with respective rates $\lambda_1, \cdots, \lambda_n$, where $\lambda_i \neq \lambda_j$ when $i \neq j$. Let $T$ be independent of these random variables and suppose that
\begin{align*}
\sum^n_{i=1} P_j = 1  \mbox{ ,where $P_j = P\{T = j\}$}
\end{align*}

The random variable $X_T$ is said to be a \impo{hyperexponential random variable}. 

\impo{Interpret $X_T$:} Consider a bin contains $n$ different types of batteries, with a type $j$ battery lasting for an exponential distributed time with rate $\lambda_j, j=1, \cdots, n$. Suppose further that $P_j$ is the proportion of batteries in the bin that are type $j$.  If a battery is randomly chosen, then the lifetime of the battery selected will have \impo{hyperexponential distribution}

To obtain the distribution function $F$ of $X= X_T$, condition on $T$. This yields,
\begin{align*}
1 - F(t) & = P\{X>t\} \\
& = \sum^n_{i=1} P\{X>t|T=i\} P\{T=i\} \\
& \sum^n_{i=1} P_i e^{-\lambda_i t}
\end{align*}

Differentiate it, we get $f(t)$, then we can get $r(t)$. 

We note that the failure rate function can also be written as
\begin{align*}
r(t) = \sum^n_{j=1} \lambda_j P\{T=j |X>t\}
\end{align*}

If $\lambda_1 < \lambda_i$, for any $i>1$, then when $t \to \infty$, we have
\begin{align*}
P\{T=1| X>t\}  \approx 1
\end{align*}

Thus we have
\begin{align*}
\lim_{t \to \infty} r(t) = \min_{i} \lambda_i
\end{align*}

i.e., the longer a battery lasts, the more likely it is a battery type with the smallest failure rate.

\subsection{Further Properties of the Exponential Distribution}
.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%