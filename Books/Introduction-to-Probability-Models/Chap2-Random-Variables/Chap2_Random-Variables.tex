%\documentclass[10pt,conference]{IEEEtran}

\documentclass[10 pt,final]{article}

\usepackage{amssymb} \usepackage{amsmath} \usepackage{amsthm} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{url} \usepackage[margin=1in]{geometry}

\usepackage{subfigure}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{definition}{Definition} \newtheorem{assumption}{Assumption} \newtheorem{example}{Example}
\newtheorem{observation}[theorem]{Observation}
%\newtheorem{theorem}{Theorem} \newtheorem{definition}{Definition} \newtheorem{remark}{Remark} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{fact}{Fact} \newtheorem{invariant}{Invariant}

\usepackage{color}
\newcounter{todocounter}
\newcommand{\todo}[1]{\stepcounter{todocounter}\textcolor{red}{to-do\#\arabic{todocounter}: #1}}
\newcommand{\impo}[1]{{\color{magenta} #1}}
\newcommand{\question}[1]{{\color{blue} #1}}



\usepackage{graphicx}
\graphicspath{{./Figures/}}

\title{Chap 2 Note: Random Variables}


\begin{document}



%\author{Huangxin Wang\thanks{Department of Computer Science, George Mason University. Fairfax, VA 22030. Email: \textsf{hwang14@gmu.edu}}}
\date{}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Random Variables and CDF}
\paragraph{Example} Suppose that independent trials, each of which results in any of $m$ possible outcomes with respective probabilities $p_1, p_2, \cdots, p_m$, $\sum^m_{i=1} p_i = 1$, are continually performed. Let $X$ denote the number of trials needed until each outcome has occurred at least once.

\paragraph{Explain} Rather than directly considering $P\{X = n\}$ we will first determine $P\{X > n\}$, the probability that at least one of the outcomes has not yet occurred after $n$ trails. Letting $A_i$ denote the event that outcome $i$ has not yet occurred after the first $n$ trails, $i=1,\cdots, m$, then 
\begin{align*}
P\{x > n\} = & P(\cup^m_{i=1} A_i) & \\
= & \sum^m_{i=1} P(A_i) - \sum \sum_{i<j} P(A_i A_j) + \cdots & \\
& + \sum \sum \sum_{i<j<k} P(A_i A_j A_k) - \cdots + (-1)^{m+1} P(A_1 \cdots A_n) & 
\end{align*} 

\impo{$P(A_i)$ is the probability that each of the first $n$ trails results in a non-$i$ outcome, and so by independence $P(A_i) = (1-p_i)^n$}

Similarly, $P(A_i A_j)$ is the probability that the first $n$ trails all result in a non-$i$ and non-$j$ outcome, and so
\begin{align*}
P(A_i A_j) = (1-p_i - p_j)^n
\end{align*}

As all of the other probabilities are similar, we see that
\begin{align*}
P\{X > n\} = & \sum^m_{i=1} (1-p_i)^n - \sum \sum_{i < j} (1- p_i - p_j)^n & \\
& + \sum \sum \sum_{i<j<k} (1-p_i-p_j-p_k)^n - \cdots &
\end{align*}

Since \impo{$P\{X=n\} = P\{X > n-1\} - P\{x>n\}$}, we see, upon using the algebraic identity \impo{$(1-a)^{n-1} - (1-a)^n = a(1-a)^{n-1}$}, that 
\begin{align*}
P\{X=n\} = & \sum^m_{i=1} p_i*(1-p_i)^{n-1} - \sum \sum_{i<j}(p_i + p_j)*(1-p_i-p_j) & \\
& +\sum \sum \sum_{i<j<k} (p_i + p_j + p_k)(1-p_i - p_j -p_k)^{n-1} -\cdots &
\end{align*}

\paragraph{CDF} The cumulative distribution function(cdf) $F(.)$ of the random variable X is defined for any real number $b$, $-\infty         < b < \infty $, by 
\begin{align*}
F(b) = P\{X \leq b\}
\end{align*}

We have 
\begin{align*}
P\{a < X  \leq b\} = F(b) - F(a), \text{for all } a < b \\
P\{X < b \} = \lim_{h \to 0^+} P\{X \leq b - h\} = \lim_{h \to 0^+} F(b-h)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete Random Variables}
\subsection{The Bernoulli Random Variable}
Suppose that a trail, or an experiment, whose outcome can be classified as either a "success" or as a "failure" is performed. If we let $X$ equal $1$ if the outcome is a success and $0$ if it is a failure, then the probability mass function of $X$ is given by
\begin{align*}
p(0) & = P\{X = 0\} = 1-p & \\
p(1) & = P\{X = 1\} = p &
\end{align*}

where $p, 0 \leq p \leq 1$, is the probability that the trail is a "success".

A random variable X is said to be a \emph{Bernoulli random variable}
if its probability mass function is given by the above equation for $p \in (0,1)$.

\subsection{The Binomial Random Variable}
Suppose that $n$ independent trails, each of which results in a "success" with probability $p$ and in a "failure" with probability $1-p$, are to be performed. If \impo{$X$ represents the number of successes that occur in the $n$ trails}, then $X$ is said to be a \impo{\emph{binomial random variable}} with parameters $(n,p)$.

The probability mass function of a binomial random variable having parameters $(n,p)$ is given by
\begin{align*}
p(i) = \binom{n}{i} p^i *(1-p)^{n-i}, i=0,1, \cdots, n
\end{align*}
where 
\begin{align*}
\binom{n}{i} = \frac{n!}{(n-i)!*i!}
\end{align*}

Note that, by the binomial theorem, the probabilities sum to one, that is,
\begin{align*}
\sum^{\infty}_{i=0} p(i) = \sum^n_{i=0} p^i*(1-p)^{n-i} = (1+(1-p))^n = 1
\end{align*}


\subsection{The Geometric Random Variable}
Suppose that independent trails, each having probability $p$ of being a success, are performed until a success occurs. If we let \impo{$X$ be the number of trails required until the first success}, then $X$ is said to be \impo{\emph{geometric random variable}} with parameter $p$. Its probability mass function is given by
\begin{align*}
p(n) = P\{X=n\} = (1-p)^{n-1}*p, n=1,2,\cdots
\end{align*}

To check that $p(n)$ is a probability mass function, we note that 
\begin{align*}
\sum^{\infty}_{n=1} p(n) = \sum^{\infty}_{n=1} (1-p)^{n-1} = 1
\end{align*}


\subsection{The Poisson Random Variable}
A random variable $X$, taking on one of the values $0,1,2, \cdots$ is said to be a Poisson random variable with parameter $\lambda$, if for some $\lambda \ge 0$, 
\begin{align*}
p(i) = P[X = i] = e^{-\lambda} \frac{\lambda^i}{i!}, i =0,1, \cdots
\end{align*}

The above equation defines a probability mass function since
\begin{align*}
\sum^{\infty}_{i=0} p(i) = e^{-\lambda} \sum^{\infty}_{i=0} \frac{\lambda^i}{i!} = e^{-\lambda}*e^{\lambda} = 1
\end{align*}

\paragraph{\impo{Approximate a Binomial Random Variable by Poisson}} An important property of the Poisson random variable is that it may e used to approximate a binomial random variable when the binomial parameter \impo{$n$ is large and $p$ is small}. To see this, suppose that $X$ is a binomial random variable with parameters $(n,p)$ and let $\lambda = n*p$. Then
\begin{align*}
P{X = i} = & \frac{n!}{(n-i)!*i!} p^i *(1-p)^{n-i} & \\
= & \frac{n!}{(n-i)!*i!}*(\lambda/n)^i(1-\lambda/n)^{n-i}&\\
= &\frac{n*(n-1)*\cdots*(n-i+1)}{n^i} *\frac{\lambda^i}{i!}*\frac{(1-\lambda/n)^n}{(1-\lambda/n)^i}
\end{align*}

For $n$ large and $p$ small, 
\begin{align*}
&(1-\lambda/n)^n \approx e^{-\lambda} \\
&\frac{n*(n-1)*\cdots*(n-i+1)}{n^i} \approx 1 \\
&(1-\lambda/n)^i \approx 1
\end{align*}

Hence for $n$ large and $p$ small, we have 
\begin{align*}
P\{X=i\} \approx e^{-\lambda}*\frac{\lambda^i}{i!}
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Continuous Random Variables}
\paragraph{Probability Density Function} $f(x)$ is called the probability density function, which is a derivative of cumulative distribution function(CDF). We have
\begin{align*}
& P\{a \leq X \leq b\} = \int_{a}^{b} f(x)dx \\
& P\{X=a\} = \int_a^a f(x)dx = 0 \\
& P\{a -\epsilon/2 \leq X \leq a+\epsilon/2\} = \int_{a-\epsilon/2}^{a+\epsilon/2} f(x)dx \approx \epsilon*f(a)
\end{align*}
In other words, the probability that \impo{$X$ will be contained in an interval of length $\epsilon$} around the point a is approximately $\epsilon *f(a)$. From this, we see that \impo{ $f(a)$ is a measure of how likely it is that the random variable will be near $a$}.


\subsection{The Uniform Random Variable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%