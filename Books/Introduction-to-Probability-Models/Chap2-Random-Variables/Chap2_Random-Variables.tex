%\documentclass[10pt,conference]{IEEEtran}

\documentclass[10 pt,final]{article}

\usepackage{amssymb} \usepackage{amsmath} \usepackage{amsthm} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{url} \usepackage[margin=1in]{geometry}

\usepackage{subfigure}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}{Lemma} \newtheorem{proposition}{Proposition} \newtheorem{corollary}{Corollary} \newtheorem{definition}{Definition} \newtheorem{assumption}{Assumption} \newtheorem{example}{Example}
\newtheorem{observation}[theorem]{Observation}
%\newtheorem{theorem}{Theorem} \newtheorem{definition}{Definition} \newtheorem{remark}{Remark} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{fact}{Fact} \newtheorem{invariant}{Invariant}

\usepackage{color}
\newcounter{todocounter}
\newcommand{\todo}[1]{\stepcounter{todocounter}\textcolor{red}{to-do\#\arabic{todocounter}: #1}}
\newcommand{\impo}[1]{{\color{magenta} #1}}
\newcommand{\question}[1]{{\color{blue} #1}}



\usepackage{graphicx}
\graphicspath{{./Figures/}}

\title{Chap 2 Note: Random Variables}


\begin{document}



%\author{Huangxin Wang\thanks{Department of Computer Science, George Mason University. Fairfax, VA 22030. Email: \textsf{hwang14@gmu.edu}}}
\date{}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Random Variables and CDF}
\paragraph{Example} Suppose that independent trials, each of which results in any of $m$ possible outcomes with respective probabilities $p_1, p_2, \cdots, p_m$, $\sum^m_{i=1} p_i = 1$, are continually performed. Let $X$ denote the number of trials needed until each outcome has occurred at least once.

\paragraph{Explain} Rather than directly considering $P\{X = n\}$ we will first determine $P\{X > n\}$, the probability that at least one of the outcomes has not yet occurred after $n$ trails. Letting $A_i$ denote the event that outcome $i$ has not yet occurred after the first $n$ trails, $i=1,\cdots, m$, then 
\begin{align*}
P\{x > n\} = & P(\cup^m_{i=1} A_i) & \\
= & \sum^m_{i=1} P(A_i) - \sum \sum_{i<j} P(A_i A_j) + \cdots & \\
& + \sum \sum \sum_{i<j<k} P(A_i A_j A_k) - \cdots + (-1)^{m+1} P(A_1 \cdots A_n) & 
\end{align*} 

\impo{$P(A_i)$ is the probability that each of the first $n$ trails results in a non-$i$ outcome, and so by independence $P(A_i) = (1-p_i)^n$}

Similarly, $P(A_i A_j)$ is the probability that the first $n$ trails all result in a non-$i$ and non-$j$ outcome, and so
\begin{align*}
P(A_i A_j) = (1-p_i - p_j)^n
\end{align*}

As all of the other probabilities are similar, we see that
\begin{align*}
P\{X > n\} = & \sum^m_{i=1} (1-p_i)^n - \sum \sum_{i < j} (1- p_i - p_j)^n & \\
& + \sum \sum \sum_{i<j<k} (1-p_i-p_j-p_k)^n - \cdots &
\end{align*}

Since \impo{$P\{X=n\} = P\{X > n-1\} - P\{x>n\}$}, we see, upon using the algebraic identity \impo{$(1-a)^{n-1} - (1-a)^n = a(1-a)^{n-1}$}, that 
\begin{align*}
P\{X=n\} = & \sum^m_{i=1} p_i*(1-p_i)^{n-1} - \sum \sum_{i<j}(p_i + p_j)*(1-p_i-p_j) & \\
& +\sum \sum \sum_{i<j<k} (p_i + p_j + p_k)(1-p_i - p_j -p_k)^{n-1} -\cdots &
\end{align*}

\paragraph{CDF} The cumulative distribution function(cdf) $F(.)$ of the random variable X is defined for any real number $b$, $-\infty         < b < \infty $, by 
\begin{align*}
F(b) = P\{X \leq b\}
\end{align*}

We have 
\begin{align*}
P\{a < X  \leq b\} = F(b) - F(a), \text{for all } a < b \\
P\{X < b \} = \lim_{h \to 0^+} P\{X \leq b - h\} = \lim_{h \to 0^+} F(b-h)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete Random Variables}
\subsection{The Bernoulli Random Variable}
Suppose that a trail, or an experiment, whose outcome can be classified as either a "success" or as a "failure" is performed. If we let $X$ equal $1$ if the outcome is a success and $0$ if it is a failure, then the probability mass function of $X$ is given by
\begin{align*}
p(0) & = P\{X = 0\} = 1-p & \\
p(1) & = P\{X = 1\} = p &
\end{align*}

where $p, 0 \leq p \leq 1$, is the probability that the trail is a "success".

A random variable X is said to be a \emph{Bernoulli random variable}
if its probability mass function is given by the above equation for $p \in (0,1)$.

\subsection{The Binomial Random Variable}
Suppose that $n$ independent trails, each of which results in a "success" with probability $p$ and in a "failure" with probability $1-p$, are to be performed. If \impo{$X$ represents the number of successes that occur in the $n$ trails}, then $X$ is said to be a \impo{\emph{binomial random variable}} with parameters $(n,p)$.

The probability mass function of a binomial random variable having parameters $(n,p)$ is given by
\begin{align*}
p(i) = \binom{n}{i} p^i *(1-p)^{n-i}, i=0,1, \cdots, n
\end{align*}
where 
\begin{align*}
\binom{n}{i} = \frac{n!}{(n-i)!*i!}
\end{align*}

Note that, by the binomial theorem, the probabilities sum to one, that is,
\begin{align*}
\sum^{\infty}_{i=0} p(i) = \sum^n_{i=0} p^i*(1-p)^{n-i} = (1+(1-p))^n = 1
\end{align*}


\subsection{The Geometric Random Variable}
Suppose that independent trails, each having probability $p$ of being a success, are performed until a success occurs. If we let \impo{$X$ be the number of trails required until the first success}, then $X$ is said to be \impo{\emph{geometric random variable}} with parameter $p$. Its probability mass function is given by
\begin{align*}
p(n) = P\{X=n\} = (1-p)^{n-1}*p, n=1,2,\cdots
\end{align*}

To check that $p(n)$ is a probability mass function, we note that 
\begin{align*}
\sum^{\infty}_{n=1} p(n) = \sum^{\infty}_{n=1} (1-p)^{n-1} = 1
\end{align*}


\subsection{The Poisson Random Variable}
A random variable $X$, taking on one of the values $0,1,2, \cdots$ is said to be a Poisson random variable with parameter $\lambda$, if for some $\lambda \ge 0$, 
\begin{align*}
p(i) = P[X = i] = e^{-\lambda} \frac{\lambda^i}{i!}, i =0,1, \cdots
\end{align*}

The above equation defines a probability mass function since
\begin{align*}
\sum^{\infty}_{i=0} p(i) = e^{-\lambda} \sum^{\infty}_{i=0} \frac{\lambda^i}{i!} = e^{-\lambda}*e^{\lambda} = 1
\end{align*}

\paragraph{\impo{Approximate a Binomial Random Variable by Poisson}} An important property of the Poisson random variable is that it may e used to approximate a binomial random variable when the binomial parameter \impo{$n$ is large and $p$ is small}. To see this, suppose that $X$ is a binomial random variable with parameters $(n,p)$ and let $\lambda = n*p$. Then
\begin{align*}
P{X = i} = & \frac{n!}{(n-i)!*i!} p^i *(1-p)^{n-i} & \\
= & \frac{n!}{(n-i)!*i!}*(\lambda/n)^i(1-\lambda/n)^{n-i}&\\
= &\frac{n*(n-1)*\cdots*(n-i+1)}{n^i} *\frac{\lambda^i}{i!}*\frac{(1-\lambda/n)^n}{(1-\lambda/n)^i}
\end{align*}

For $n$ large and $p$ small, 
\begin{align*}
&(1-\lambda/n)^n \approx e^{-\lambda} \\
&\frac{n*(n-1)*\cdots*(n-i+1)}{n^i} \approx 1 \\
&(1-\lambda/n)^i \approx 1
\end{align*}

Hence for $n$ large and $p$ small, we have 
\begin{align*}
P\{X=i\} \approx e^{-\lambda}*\frac{\lambda^i}{i!}
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Continuous Random Variables}
\paragraph{Probability Density Function} $f(x)$ is called the probability density function, which is a derivative of cumulative distribution function(CDF). We have
\begin{align*}
& P\{a \leq X \leq b\} = \int_{a}^{b} f(x)dx \\
& P\{X=a\} = \int_a^a f(x)dx = 0 \\
& P\{a -\epsilon/2 \leq X \leq a+\epsilon/2\} = \int_{a-\epsilon/2}^{a+\epsilon/2} f(x)dx \approx \epsilon*f(a)
\end{align*}
In other words, the probability that \impo{$X$ will be contained in an interval of length $\epsilon$} around the point a is approximately $\epsilon *f(a)$. From this, we see that \impo{ $f(a)$ is a measure of how likely it is that the random variable will be near $a$}.


\subsection{The Uniform Random Variable}
A random variable is said to be \impo{uniformly distributed} over the interval $(0,1)$ if its probability density is given by

\begin{align*}
f(x) =
\begin{cases}
1 &0 < x < 1\\ 
0 &otherwise.
\end{cases}
\end{align*}

Note that the preceding is a density function since $f(x) \geq 0$ and
\begin{align*}
\int_{-\infty}^{\infty} f(x)dx = \int_0^1 = 1
\end{align*}

In general, we say that $X$ is a uniform random variable on the interval $(\alpha, \beta)$ if its probability density function is given by
\begin{align*}
f(x) =
\begin{cases}
\frac{1}{\beta - \alpha} & \mbox{if } \alpha < x < \beta \\
0 & otherwise
\end{cases}
\end{align*}


\subsection{Exponential Random Variable}
A continuous random variable whose probability density function is given, for some $\lambda > 0$, by
\begin{align*}
f(x) =
\begin{cases}
\lambda * e^{-\lambda*x} & \mbox{if } x \ge 0 \\
0 & \mbox{if } x <0
\end{cases}
\end{align*}

is said to be an \impo{exponential random variable} with parameter $\lambda$.

\subsection{Gamma Random Variables}
A continuous random variable whose density is given by
\begin{align*}
f(x) = 
\begin{cases}
\lambda e^{-\lambda x} (\lambda x)^{\alpha-1} & \mbox{ if } x \ge 0 \\
0 & \mbox{ if } x<0
\end{cases}
\end{align*}

for some $\lambda > 0, \alpha >0$ is said to be a \impo{gamma random variable} with parameter $\alpha, \lambda$. The quantity $\Gamma(\alpha)$ is called the gamma function and is defined by 
\begin{align*}
\Gamma(\alpha) = \int_0^{\infty} e^{-x} x^{\alpha-1} dx
\end{align*}

\question{It is easy to show by induction that for integral $\alpha$, say $\alpha = n$,} 
\begin{align*}
\Gamma(n) = (n-1)!
\end{align*}

\subsection{Normal Random Variables}
We say that $X$ is a \impo{normal random variable}(or simply that $X$ is \impo{normal distributed}) with parameters $\mu$ and $\sigma^2$ of $X$ is given by
\begin{align*}
f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x-\mu)^2/2 \sigma^2}, -\infty < x < \infty
\end{align*}

The density function is a \impo{bell-shaped} curve that is \impo{symmetric} around $\mu$.

An important fact about normal random variables is that if $X$ is normally distributed with parameter $\mu$ and $\sigma^2$ then $Y = \alpha *X + \beta$ is normally distributed with parameters $\alpha*\mu + \beta$ and \impo{$\alpha^2 \sigma^2$}. 

\begin{proof}
Suppose first that $\alpha >0$ and note that $F_Y(.)$, the cumulative distribution function of the random variable $Y$, is given by
\begin{align*}
F_Y(a) = & P\{ Y \leq a \} & \\
= & P\{\alpha X + \beta \leq a\} & \\
= & P\{ X \leq \frac{a - \beta}{\alpha}\} & \\
= & F_X(\frac{a-\beta}{\alpha}) & \\
= & \int_{-\infty}^{(a-\beta)/\alpha} \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x-\mu)^2/2 \sigma^2} dx &\\
= &\int_{-\infty}^a \frac{1}{\sqrt{2 \pi} \alpha \sigma} exp\{ \frac{-(v-(\alpha \mu + \beta))^2}{2\alpha^2 \sigma^2}\} dv &\\
\end{align*}
where the last equality is obtained by the change in variables $v = \alpha x + \beta$. However, since $F_Y(a) = \int_{-\infty}^a f_Y(v) dv $, it follows from the Equation that the profit density function $f_Y(.)$ is given by 
\begin{align*}
f_Y(v) = \frac{1}{\sqrt{2 \pi} \alpha \sigma} exp\{ \frac{-(v-(\alpha \mu + \beta))^2}{2(\alpha \sigma)^2}\}, -\infty < v < \infty
\end{align*}

Hence, $Y$ is normally distributed with parameters $\alpha \mu + \beta$ and $(\alpha \mu)^2$. A similar result is also true when $\alpha < 0$.
\end{proof}

\paragraph{Implementation} One implementation of the preceding result is that if $X$ is normally distributed with parameters $\mu$ and $\sigma^2$ then $Y=\frac{X - \mu}{\sigma}$ is normally distributed with parameters $0$ and $1$. Such a random variable $Y$ is said to have the \impo{standard} or \impo{unit normal distribution}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expectation of a Random Variable}
\paragraph{Expected Value} a weighted average of the possible value that $X$ can take on, each value being weighted by the probability that $X$ could be.

\subsection{The Discrete Case}
\begin{itemize}
\item \textbf{Bernoulli Random Variable} p
\item \textbf{Binomial Random Variable} np
\item \textbf{Geometric Random Variable} $\frac{1}{p}$
\item \textbf{Poisson Random Variable} $\lambda$
\end{itemize}

\subsection{The Continuous Case}
\paragraph{Example(Expectation of an Exponential Random Variable)} Let $X$ be exponentially distributed with parameter $\lambda$, calculated $E[X]$. 
\paragraph{Solution:} 
\begin{align*}
E[X] & = \int_0^{\infty} x \lambda e^{\lambda X} dx 
\end{align*}

Integrating by parts ($dv = \lambda e^{\lambda x}$, $u=x$) yields, 
\begin{align*}
E[X] & = -x e^{-\lambda x}|^{\infty}_{0} + \int_0^{\infty} e^{-\lambda x} dx \\
& = 0 - \frac{e^{-\lambda x}}{\lambda}|^\infty_{0}\\
& = \frac{1}{\lambda} \\
\end{align*}

\subsection{Expectation of a Function of a Random Variable}
\paragraph{Example} Let $X$ be uniformly distributed over $(0,1)$. Calculate $E[X^3]$.

\textbf{Solution:}
\begin{align*}
E[X^3] = \int_0^{1} x^3 dx = \frac{1}{4}
\end{align*}


\paragraph{\impo{Moment}} The expected value of a random variable $X$, $E[X]$, is also referred to as the mean or the first moment of $X$. The quantity $E[X^n], n \geq 1$, is called the \impo{$n$-th moment} of $X$. We have
\begin{align*}
E[X^n] = 
\begin{cases}
\sum_{x:p(x) > 0} x^n p(x) & \mbox{ if $X$ is discrete} \\
\int_{-\infty}^{\infty} x^n f(x) dx & \mbox{ if $X$ is continuous}
\end{cases}
\end{align*}

\paragraph{Variance} 
\begin{align*}
Var(X) = E[X^2] - (E[x])^2
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Jointly Discrete Random Variables}
\subsection{Joint Distribution Functions}
\paragraph{Example} Calculate the expected sum obtained when three fair dice are rolled.

\textbf{Solution:} Let $X$ denote the sum obtained. Then $X = X_1 + X_2 + X_3$ where $X_i$ represents the value of the $i$-th die. Thus,
\begin{align*}
E[X] = E[X_1] + E[X_2] + E[X_3] = 3*(7/2) = 21/2
\end{align*}

\paragraph{Example} At a party $n$ men throw their hats into the center of a room. The hats are mixed up and each man randomly selects one. Find the expected number of men who select their own hats. 

\textbf{Solution:} Letting $X$ denote the number of men that selects their own hats, we can best compute $E[X]$ by noting that 
\begin{align*}
X = X_1 + X_2 + \cdots + X_N
\end{align*}
where
\begin{align*}
X_i =
\begin{cases}
1 & \mbox{ if the ith man selects his own hat} \\
0 & \mbox{ otherwise}
\end{cases}
\end{align*}

Now, because the $i$-th man is equally likely to select any of the $N$ hats, it follows that 
\begin{align*}
P\{X_i=1\} = P\{\mbox{the ith man selects his own hat}\} = \frac{1}{N}
\end{align*}

and so 
\begin{align*}
E[X_i] = 1*Pr\{X_i = 1\} + 0 * Pr\{X_i = 0\}
\end{align*}

Hence, we have $E[X] = 1$. Hence now matter how many people are at the party, on the average exactly one of the men will select his own hat.



\paragraph{Example} Suppose that there are $25$ different types of coupons and suppose that each time one obtains a coupon, it is equally likely to be any one of the $25$ types. Compare the expected number of different types that are contained in a set of $10$ coupons. 

\textbf{Solution:} Let $X$ denote the number of different types in the set of $10$ coupons. We compare $E[X]$ by using the representation $E[X] = E[X_1] + \cdots + E[X_{25}]$.

where 
\begin{align*}
X_i =
\begin{cases}
1 & \mbox{if at least one type $i$ coupon is in the set of 10} \\
0 & \mbox{otherwise} 
\end{cases}
\end{align*}
Now,

\begin{align*}
E[X_i] & = P\{X_i = \} \\
& = P\{\mbox{At least one type $i$ coupon is in the set of $10$}\} \\
& = 1 - P\{\mbox{no type $i$ coupons are in the set of $10$}\} \\
& = 1 - (\frac{24}{25})^2
\end{align*}

Therefore, $E[X] = 25*(1-(\frac{24}{25})^25)$.


\subsection{Covariance and Variance of Sums of Random Variables}
The covariance of any two random variables $X$ and $Y$, denoted by $cov(X,T)$, is defined by
\begin{align*}
Cov(X,Y) & = E[(x-E[X])(Y-E[Y])] \\
& = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\
& = E[XY] -E[X]E[Y] - E[Y]E[X] + E[X]E[Y] \\
& = E[XY] - E[X]E[Y]
\end{align*}
Note that if $X$ and $Y$ are \impo{independent}, then it follows that \impo{$Cov(X,Y) = 0$}.

\paragraph{Properties of Covariance} For any random variables $X, Y, 
Z$ and constant $c$
\begin{itemize}
\item $Cov(X,X) = Var(X)$
\item $Cov(cX,Y) = cCov(X,Y)$
\item $Cov(X,Y+Z) = Cov(X,Y) + Cov(X,Z)$
\end{itemize}

For the last property, we can easily generalizes to give the following result
\impo{
\begin{align*}
cov(\sum^n_{i=1} X_i, \sum^m_{j=1} Y_j) = \sum^n_{i=1} \sum^m_{j=1} Cov(X_i, Y_j)
\end{align*}}

A useful expression for the variance of the sum of random variables can be obtained as follows
\begin{align*}
var(\sum^n_{i=1} X_i) & = Cov(\sum^n_{i=1} X_i, \sum^n_{j=1} X_j) \\
& = \sum^n_{i=1} \sum^n_{j=1} Cov(X_i, X_j) \\
& = \sum^n_{i=1} Cov(X_i,X_i) + \sum^n_{i=1} \sum_{j \neq i} Cov(X_i, X_j) \\
& = \sum^n_{i=1} Var(X_i) + 2 \sum^n_{i=1} \sum_{j < i} Cov(X_i, X_j)
\end{align*}

If $X_i, i=1,2, \cdots, n$ are independent random variables, then we can get
\begin{align*}
Var(\sum^n_{i=1} x_i) = \sum^n_{i=1} Var(X_i)
\end{align*}

\begin{proposition}
Suppose that $X_1, \cdots, X_n$ are independent and identically distributed with expected value $\mu$ and variance $\sigma^2$. Then\\
(a) $E[\bar{X}] = \mu$ \\
(b) $Var(\bar{X}] = \sigma^2/n$ \\ 
(c) $Cov(\bar{X}, X_i - \bar{X}) = 0, i=1,\cdots, n.$
\end{proposition}

\paragraph{Example} Compute the variance of a \impo{Binomial Random Variable} $X$ with parameters $n$ and $p$.

\textbf{Solution:} Since such a random variable represents the number of successes in $n$ independent trails when each trail has a common probability $p$ of being a success, we may write
\begin{align*}
X = X_1 + X_2 + \cdots X_n
\end{align*}

where $X_i$ are independent \impo{Bernoulli random variables} such that 
\begin{align*}
X_i = 
\begin{cases}
1 & \mbox{ if the ithe trial is a success} \\
0 & \mbox{ otherwise}
\end{cases}
\end{align*}

Hence we have
\begin{align*}
Var(X) = Var(X_1) + Var(X_2) + \cdots + Var(X_n)
\end{align*}

since $Var(X_i) = E[(X_i)^2] - (E[X_i])^2 = p - p^2$ since $(X_i)^2 = X_i$.

And thus $Var(X) = np(1-p)$.

\paragraph{\impo{Hypergeometric}} The random variable $\sum^n_{i=1} X_i$ can be thought of representing the number of white balls obtained when $n$ balls are randomly selected from a population consisting of $Np$ white and $N-Np$ black balls. Such a random variable is called \impo{hypergeometric} and has a probability mass function given by 
\begin{align*}
P\{\sum^n_{i=1} X_i = k\} = \frac{\binom{Np}{k} \binom{N-Np}{n-k}}{\binom{N}{n}}
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%