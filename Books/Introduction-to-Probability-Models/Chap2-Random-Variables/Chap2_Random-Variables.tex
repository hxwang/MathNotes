%\documentclass[10pt,conference]{IEEEtran}

\documentclass[10 pt,final]{article}

\usepackage{amssymb} \usepackage{amsmath} \usepackage{amsthm} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{url} \usepackage[margin=1in]{geometry}

\usepackage{subfigure}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}{Lemma} \newtheorem{proposition}{Proposition} \newtheorem{corollary}{Corollary} \newtheorem{definition}{Definition} \newtheorem{assumption}{Assumption} \newtheorem{example}{Example}
\newtheorem{observation}[theorem]{Observation}
%\newtheorem{theorem}{Theorem} \newtheorem{definition}{Definition} \newtheorem{remark}{Remark} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{fact}{Fact} \newtheorem{invariant}{Invariant}

\usepackage{color}
\newcounter{todocounter}
\newcommand{\todo}[1]{\stepcounter{todocounter}\textcolor{red}{to-do\#\arabic{todocounter}: #1}}
\newcommand{\impo}[1]{{\color{magenta} #1}}
\newcommand{\question}[1]{{\color{blue} #1}}



\usepackage{graphicx}
\graphicspath{{./Figures/}}

\title{Chap 2 Note: Random Variables}


\begin{document}



%\author{Huangxin Wang\thanks{Department of Computer Science, George Mason University. Fairfax, VA 22030. Email: \textsf{hwang14@gmu.edu}}}
\date{}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Random Variables and CDF}
\paragraph{Example} Suppose that independent trials, each of which results in any of $m$ possible outcomes with respective probabilities $p_1, p_2, \cdots, p_m$, $\sum^m_{i=1} p_i = 1$, are continually performed. Let $X$ denote the number of trials needed until each outcome has occurred at least once.

\paragraph{Explain} Rather than directly considering $P\{X = n\}$ we will first determine $P\{X > n\}$, the probability that at least one of the outcomes has not yet occurred after $n$ trails. Letting $A_i$ denote the event that outcome $i$ has not yet occurred after the first $n$ trails, $i=1,\cdots, m$, then 
\begin{align*}
P\{x > n\} = & P(\cup^m_{i=1} A_i) & \\
= & \sum^m_{i=1} P(A_i) - \sum \sum_{i<j} P(A_i A_j) + \cdots & \\
& + \sum \sum \sum_{i<j<k} P(A_i A_j A_k) - \cdots + (-1)^{m+1} P(A_1 \cdots A_n) & 
\end{align*} 

\impo{$P(A_i)$ is the probability that each of the first $n$ trails results in a non-$i$ outcome, and so by independence $P(A_i) = (1-p_i)^n$}

Similarly, $P(A_i A_j)$ is the probability that the first $n$ trails all result in a non-$i$ and non-$j$ outcome, and so
\begin{align*}
P(A_i A_j) = (1-p_i - p_j)^n
\end{align*}

As all of the other probabilities are similar, we see that
\begin{align*}
P\{X > n\} = & \sum^m_{i=1} (1-p_i)^n - \sum \sum_{i < j} (1- p_i - p_j)^n & \\
& + \sum \sum \sum_{i<j<k} (1-p_i-p_j-p_k)^n - \cdots &
\end{align*}

Since \impo{$P\{X=n\} = P\{X > n-1\} - P\{x>n\}$}, we see, upon using the algebraic identity \impo{$(1-a)^{n-1} - (1-a)^n = a(1-a)^{n-1}$}, that 
\begin{align*}
P\{X=n\} = & \sum^m_{i=1} p_i*(1-p_i)^{n-1} - \sum \sum_{i<j}(p_i + p_j)*(1-p_i-p_j) & \\
& +\sum \sum \sum_{i<j<k} (p_i + p_j + p_k)(1-p_i - p_j -p_k)^{n-1} -\cdots &
\end{align*}

\paragraph{CDF} The cumulative distribution function(cdf) $F(.)$ of the random variable X is defined for any real number $b$, $-\infty         < b < \infty $, by 
\begin{align*}
F(b) = P\{X \leq b\}
\end{align*}

We have 
\begin{align*}
P\{a < X  \leq b\} = F(b) - F(a), \text{for all } a < b \\
P\{X < b \} = \lim_{h \to 0^+} P\{X \leq b - h\} = \lim_{h \to 0^+} F(b-h)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete Random Variables}
\subsection{The Bernoulli Random Variable}
Suppose that a trail, or an experiment, whose outcome can be classified as either a "success" or as a "failure" is performed. If we let $X$ equal $1$ if the outcome is a success and $0$ if it is a failure, then the probability mass function of $X$ is given by
\begin{align*}
p(0) & = P\{X = 0\} = 1-p & \\
p(1) & = P\{X = 1\} = p &
\end{align*}

where $p, 0 \leq p \leq 1$, is the probability that the trail is a "success".

A random variable X is said to be a \emph{Bernoulli random variable}
if its probability mass function is given by the above equation for $p \in (0,1)$.

\subsection{The Binomial Random Variable}
Suppose that $n$ independent trails, each of which results in a "success" with probability $p$ and in a "failure" with probability $1-p$, are to be performed. If \impo{$X$ represents the number of successes that occur in the $n$ trails}, then $X$ is said to be a \impo{\emph{binomial random variable}} with parameters $(n,p)$.

The probability mass function of a binomial random variable having parameters $(n,p)$ is given by
\begin{align*}
p(i) = \binom{n}{i} p^i *(1-p)^{n-i}, i=0,1, \cdots, n
\end{align*}
where 
\begin{align*}
\binom{n}{i} = \frac{n!}{(n-i)!*i!}
\end{align*}

Note that, by the binomial theorem, the probabilities sum to one, that is,
\begin{align*}
\sum^{\infty}_{i=0} p(i) = \sum^n_{i=0} p^i*(1-p)^{n-i} = (1+(1-p))^n = 1
\end{align*}


\subsection{The Geometric Random Variable}
Suppose that independent trails, each having probability $p$ of being a success, are performed until a success occurs. If we let \impo{$X$ be the number of trails required until the first success}, then $X$ is said to be \impo{\emph{geometric random variable}} with parameter $p$. Its probability mass function is given by
\begin{align*}
p(n) = P\{X=n\} = (1-p)^{n-1}*p, n=1,2,\cdots
\end{align*}

To check that $p(n)$ is a probability mass function, we note that 
\begin{align*}
\sum^{\infty}_{n=1} p(n) = \sum^{\infty}_{n=1} (1-p)^{n-1} = 1
\end{align*}


\subsection{The Poisson Random Variable}
A random variable $X$, taking on one of the values $0,1,2, \cdots$ is said to be a Poisson random variable with parameter $\lambda$, if for some $\lambda \ge 0$, 
\begin{align*}
p(i) = P[X = i] = e^{-\lambda} \frac{\lambda^i}{i!}, i =0,1, \cdots
\end{align*}

The above equation defines a probability mass function since
\begin{align*}
\sum^{\infty}_{i=0} p(i) = e^{-\lambda} \sum^{\infty}_{i=0} \frac{\lambda^i}{i!} = e^{-\lambda}*e^{\lambda} = 1
\end{align*}

\paragraph{\impo{Approximate a Binomial Random Variable by Poisson}} An important property of the Poisson random variable is that it may e used to approximate a binomial random variable when the binomial parameter \impo{$n$ is large and $p$ is small}. To see this, suppose that $X$ is a binomial random variable with parameters $(n,p)$ and let $\lambda = n*p$. Then
\begin{align*}
P{X = i} = & \frac{n!}{(n-i)!*i!} p^i *(1-p)^{n-i} & \\
= & \frac{n!}{(n-i)!*i!}*(\lambda/n)^i(1-\lambda/n)^{n-i}&\\
= &\frac{n*(n-1)*\cdots*(n-i+1)}{n^i} *\frac{\lambda^i}{i!}*\frac{(1-\lambda/n)^n}{(1-\lambda/n)^i}
\end{align*}

For $n$ large and $p$ small, 
\begin{align*}
&(1-\lambda/n)^n \approx e^{-\lambda} \\
&\frac{n*(n-1)*\cdots*(n-i+1)}{n^i} \approx 1 \\
&(1-\lambda/n)^i \approx 1
\end{align*}

Hence for $n$ large and $p$ small, we have 
\begin{align*}
P\{X=i\} \approx e^{-\lambda}*\frac{\lambda^i}{i!}
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Continuous Random Variables}
\paragraph{Probability Density Function} $f(x)$ is called the probability density function, which is a derivative of cumulative distribution function(CDF). We have
\begin{align*}
& P\{a \leq X \leq b\} = \int_{a}^{b} f(x)dx \\
& P\{X=a\} = \int_a^a f(x)dx = 0 \\
& P\{a -\epsilon/2 \leq X \leq a+\epsilon/2\} = \int_{a-\epsilon/2}^{a+\epsilon/2} f(x)dx \approx \epsilon*f(a)
\end{align*}
In other words, the probability that \impo{$X$ will be contained in an interval of length $\epsilon$} around the point a is approximately $\epsilon *f(a)$. From this, we see that \impo{ $f(a)$ is a measure of how likely it is that the random variable will be near $a$}.


\subsection{The Uniform Random Variable}
A random variable is said to be \impo{uniformly distributed} over the interval $(0,1)$ if its probability density is given by

\begin{align*}
f(x) =
\begin{cases}
1 &0 < x < 1\\ 
0 &otherwise.
\end{cases}
\end{align*}

Note that the preceding is a density function since $f(x) \geq 0$ and
\begin{align*}
\int_{-\infty}^{\infty} f(x)dx = \int_0^1 = 1
\end{align*}

In general, we say that $X$ is a uniform random variable on the interval $(\alpha, \beta)$ if its probability density function is given by
\begin{align*}
f(x) =
\begin{cases}
\frac{1}{\beta - \alpha} & \mbox{if } \alpha < x < \beta \\
0 & otherwise
\end{cases}
\end{align*}


\subsection{Exponential Random Variable}
A continuous random variable whose probability density function is given, for some $\lambda > 0$, by
\begin{align*}
f(x) =
\begin{cases}
\lambda * e^{-\lambda*x} & \mbox{if } x \ge 0 \\
0 & \mbox{if } x <0
\end{cases}
\end{align*}

is said to be an \impo{exponential random variable} with parameter $\lambda$.

\subsection{Gamma Random Variables}
A continuous random variable whose density is given by
\begin{align*}
f(x) = 
\begin{cases}
\lambda e^{-\lambda x} (\lambda x)^{\alpha-1} & \mbox{ if } x \ge 0 \\
0 & \mbox{ if } x<0
\end{cases}
\end{align*}

for some $\lambda > 0, \alpha >0$ is said to be a \impo{gamma random variable} with parameter $\alpha, \lambda$. The quantity $\Gamma(\alpha)$ is called the gamma function and is defined by 
\begin{align*}
\Gamma(\alpha) = \int_0^{\infty} e^{-x} x^{\alpha-1} dx
\end{align*}

\question{It is easy to show by induction that for integral $\alpha$, say $\alpha = n$,} 
\begin{align*}
\Gamma(n) = (n-1)!
\end{align*}

\subsection{Normal Random Variables}
We say that $X$ is a \impo{normal random variable}(or simply that $X$ is \impo{normal distributed}) with parameters $\mu$ and $\sigma^2$ of $X$ is given by
\begin{align*}
f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x-\mu)^2/2 \sigma^2}, -\infty < x < \infty
\end{align*}

The density function is a \impo{bell-shaped} curve that is \impo{symmetric} around $\mu$.

An important fact about normal random variables is that if $X$ is normally distributed with parameter $\mu$ and $\sigma^2$ then $Y = \alpha *X + \beta$ is normally distributed with parameters $\alpha*\mu + \beta$ and \impo{$\alpha^2 \sigma^2$}. 

\begin{proof}
Suppose first that $\alpha >0$ and note that $F_Y(.)$, the cumulative distribution function of the random variable $Y$, is given by
\begin{align*}
F_Y(a) = & P\{ Y \leq a \} & \\
= & P\{\alpha X + \beta \leq a\} & \\
= & P\{ X \leq \frac{a - \beta}{\alpha}\} & \\
= & F_X(\frac{a-\beta}{\alpha}) & \\
= & \int_{-\infty}^{(a-\beta)/\alpha} \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x-\mu)^2/2 \sigma^2} dx &\\
= &\int_{-\infty}^a \frac{1}{\sqrt{2 \pi} \alpha \sigma} exp\{ \frac{-(v-(\alpha \mu + \beta))^2}{2\alpha^2 \sigma^2}\} dv &\\
\end{align*}
where the last equality is obtained by the change in variables $v = \alpha x + \beta$. However, since $F_Y(a) = \int_{-\infty}^a f_Y(v) dv $, it follows from the Equation that the profit density function $f_Y(.)$ is given by 
\begin{align*}
f_Y(v) = \frac{1}{\sqrt{2 \pi} \alpha \sigma} exp\{ \frac{-(v-(\alpha \mu + \beta))^2}{2(\alpha \sigma)^2}\}, -\infty < v < \infty
\end{align*}

Hence, $Y$ is normally distributed with parameters $\alpha \mu + \beta$ and $(\alpha \mu)^2$. A similar result is also true when $\alpha < 0$.
\end{proof}

\paragraph{Implementation} One implementation of the preceding result is that if $X$ is normally distributed with parameters $\mu$ and $\sigma^2$ then $Y=\frac{X - \mu}{\sigma}$ is normally distributed with parameters $0$ and $1$. Such a random variable $Y$ is said to have the \impo{standard} or \impo{unit normal distribution}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expectation of a Random Variable}
\paragraph{Expected Value} a weighted average of the possible value that $X$ can take on, each value being weighted by the probability that $X$ could be.

\subsection{The Discrete Case}
\begin{itemize}
\item \textbf{Bernoulli Random Variable} p
\item \textbf{Binomial Random Variable} np
\item \textbf{Geometric Random Variable} $\frac{1}{p}$
\item \textbf{Poisson Random Variable} $\lambda$
\end{itemize}

\subsection{The Continuous Case}
\paragraph{Example(Expectation of an Exponential Random Variable)} Let $X$ be exponentially distributed with parameter $\lambda$, calculated $E[X]$. 
\paragraph{Solution:} 
\begin{align*}
E[X] & = \int_0^{\infty} x \lambda e^{\lambda X} dx 
\end{align*}

Integrating by parts ($dv = \lambda e^{\lambda x}$, $u=x$) yields, 
\begin{align*}
E[X] & = -x e^{-\lambda x}|^{\infty}_{0} + \int_0^{\infty} e^{-\lambda x} dx \\
& = 0 - \frac{e^{-\lambda x}}{\lambda}|^\infty_{0}\\
& = \frac{1}{\lambda} \\
\end{align*}

\subsection{Expectation of a Function of a Random Variable}
\paragraph{Example} Let $X$ be uniformly distributed over $(0,1)$. Calculate $E[X^3]$.

\textbf{Solution:}
\begin{align*}
E[X^3] = \int_0^{1} x^3 dx = \frac{1}{4}
\end{align*}


\paragraph{\impo{Moment}} The expected value of a random variable $X$, $E[X]$, is also referred to as the mean or the first moment of $X$. The quantity $E[X^n], n \geq 1$, is called the \impo{$n$-th moment} of $X$. We have
\begin{align*}
E[X^n] = 
\begin{cases}
\sum_{x:p(x) > 0} x^n p(x) & \mbox{ if $X$ is discrete} \\
\int_{-\infty}^{\infty} x^n f(x) dx & \mbox{ if $X$ is continuous}
\end{cases}
\end{align*}

\paragraph{Variance} 
\begin{align*}
Var(X) = E[X^2] - (E[x])^2
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Jointly Discrete Random Variables}
\subsection{Joint Distribution Functions}
\paragraph{Example} Calculate the expected sum obtained when three fair dice are rolled.

\textbf{Solution:} Let $X$ denote the sum obtained. Then $X = X_1 + X_2 + X_3$ where $X_i$ represents the value of the $i$-th die. Thus,
\begin{align*}
E[X] = E[X_1] + E[X_2] + E[X_3] = 3*(7/2) = 21/2
\end{align*}

\paragraph{Example} At a party $n$ men throw their hats into the center of a room. The hats are mixed up and each man randomly selects one. Find the expected number of men who select their own hats. 

\textbf{Solution:} Letting $X$ denote the number of men that selects their own hats, we can best compute $E[X]$ by noting that 
\begin{align*}
X = X_1 + X_2 + \cdots + X_N
\end{align*}
where
\begin{align*}
X_i =
\begin{cases}
1 & \mbox{ if the ith man selects his own hat} \\
0 & \mbox{ otherwise}
\end{cases}
\end{align*}

Now, because the $i$-th man is equally likely to select any of the $N$ hats, it follows that 
\begin{align*}
P\{X_i=1\} = P\{\mbox{the ith man selects his own hat}\} = \frac{1}{N}
\end{align*}

and so 
\begin{align*}
E[X_i] = 1*Pr\{X_i = 1\} + 0 * Pr\{X_i = 0\}
\end{align*}

Hence, we have $E[X] = 1$. Hence now matter how many people are at the party, on the average exactly one of the men will select his own hat.



\paragraph{Example} Suppose that there are $25$ different types of coupons and suppose that each time one obtains a coupon, it is equally likely to be any one of the $25$ types. Compare the expected number of different types that are contained in a set of $10$ coupons. 

\textbf{Solution:} Let $X$ denote the number of different types in the set of $10$ coupons. We compare $E[X]$ by using the representation $E[X] = E[X_1] + \cdots + E[X_{25}]$.

where 
\begin{align*}
X_i =
\begin{cases}
1 & \mbox{if at least one type $i$ coupon is in the set of 10} \\
0 & \mbox{otherwise} 
\end{cases}
\end{align*}
Now,

\begin{align*}
E[X_i] & = P\{X_i = \} \\
& = P\{\mbox{At least one type $i$ coupon is in the set of $10$}\} \\
& = 1 - P\{\mbox{no type $i$ coupons are in the set of $10$}\} \\
& = 1 - (\frac{24}{25})^2
\end{align*}

Therefore, $E[X] = 25*(1-(\frac{24}{25})^25)$.


\subsection{Covariance and Variance of Sums of Random Variables}
The covariance of any two random variables $X$ and $Y$, denoted by $cov(X,T)$, is defined by
\begin{align*}
Cov(X,Y) & = E[(x-E[X])(Y-E[Y])] \\
& = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\
& = E[XY] -E[X]E[Y] - E[Y]E[X] + E[X]E[Y] \\
& = E[XY] - E[X]E[Y]
\end{align*}
Note that if $X$ and $Y$ are \impo{independent}, then it follows that \impo{$Cov(X,Y) = 0$}.

\paragraph{Properties of Covariance} For any random variables $X, Y, 
Z$ and constant $c$
\begin{itemize}
\item $Cov(X,X) = Var(X)$
\item $Cov(cX,Y) = cCov(X,Y)$
\item $Cov(X,Y+Z) = Cov(X,Y) + Cov(X,Z)$
\end{itemize}

For the last property, we can easily generalizes to give the following result
\impo{
\begin{align*}
cov(\sum^n_{i=1} X_i, \sum^m_{j=1} Y_j) = \sum^n_{i=1} \sum^m_{j=1} Cov(X_i, Y_j)
\end{align*}}

A useful expression for the variance of the sum of random variables can be obtained as follows
\begin{align*}
var(\sum^n_{i=1} X_i) & = Cov(\sum^n_{i=1} X_i, \sum^n_{j=1} X_j) \\
& = \sum^n_{i=1} \sum^n_{j=1} Cov(X_i, X_j) \\
& = \sum^n_{i=1} Cov(X_i,X_i) + \sum^n_{i=1} \sum_{j \neq i} Cov(X_i, X_j) \\
& = \sum^n_{i=1} Var(X_i) + 2 \sum^n_{i=1} \sum_{j < i} Cov(X_i, X_j)
\end{align*}

If $X_i, i=1,2, \cdots, n$ are independent random variables, then we can get
\begin{align*}
Var(\sum^n_{i=1} x_i) = \sum^n_{i=1} Var(X_i)
\end{align*}

\begin{proposition}
Suppose that $X_1, \cdots, X_n$ are independent and identically distributed with expected value $\mu$ and variance $\sigma^2$. Then\\
(a) $E[\bar{X}] = \mu$ \\
(b) $Var(\bar{X}] = \sigma^2/n$ \\ 
(c) $Cov(\bar{X}, X_i - \bar{X}) = 0, i=1,\cdots, n.$
\end{proposition}

\paragraph{Example} Compute the variance of a \impo{Binomial Random Variable} $X$ with parameters $n$ and $p$.

\textbf{Solution:} Since such a random variable represents the number of successes in $n$ independent trails when each trail has a common probability $p$ of being a success, we may write
\begin{align*}
X = X_1 + X_2 + \cdots X_n
\end{align*}

where $X_i$ are independent \impo{Bernoulli random variables} such that 
\begin{align*}
X_i = 
\begin{cases}
1 & \mbox{ if the ithe trial is a success} \\
0 & \mbox{ otherwise}
\end{cases}
\end{align*}

Hence we have
\begin{align*}
Var(X) = Var(X_1) + Var(X_2) + \cdots + Var(X_n)
\end{align*}

since $Var(X_i) = E[(X_i)^2] - (E[X_i])^2 = p - p^2$ since $(X_i)^2 = X_i$.

And thus $Var(X) = np(1-p)$.



\paragraph{\impo{Hypergeometric}} The random variable $\sum^n_{i=1} X_i$ can be thought of representing the number of white balls obtained when $n$ balls are randomly selected from a population consisting of $Np$ white and $N-Np$ black balls. Such a random variable is called \impo{hypergeometric} and has a probability mass function given by 
\begin{align*}
P\{\sum^n_{i=1} X_i = k\} = \frac{\binom{Np}{k} \binom{N-Np}{n-k}}{\binom{N}{n}}
\end{align*}

\subsection{\impo{Distribution of X+Y}}
We consider the distribution of $X+Y$ from the distributions of $X$ and $Y$ when $X$ and $Y$ are independent. Suppose first that $X$ and $Y$ are continuous, $X$ having probability density $f$ and $Y$ having probability density $g$. Then letting $F_{X+Y}(a)$ be the cumulative distribution function of $X+Y$, we have
\begin{align*}
F_{X+Y}(a) & = P{X+Y \leq a} \\
& = \int \int+{X+ Y \leq a} f(x)g(y) dx dy \\
& = \int_{\infty}^\infty \int_\infty^{a-y} f(x) g(y) dxdy \\
& = \int_{-\infty}^\infty (\int_{-\infty}^{a-y}dx) g(y)dy \\
& = \int_{-\infty}^\infty F_X(a-y) g(y)dy
\end{align*}

The cumulative distribution function $F+{X+Y}$ is called the \impo{convolution} of the distributions $F_X$ and $F_T$(the cumulative distributions functions of $X$ and $Y$ respectively).

By differentiating the equation, we obtain the probability density function $f_{X+Y} (a)$ of $X+Y$ given by
\begin{align*}
f+{X+Y})(a) & = \frac{d}{da} \int_{-\infty}^\infty F_X(a-y) g(y) dy \\
& = \int_{-\infty}^\infty \frac{d}{da} (F_X(a-y))g(y)dy \\
& = \impo{ \int_{-\infty}^\infty f(a-y) g(y) dy}
\end{align*}


\paragraph{Example} If $X$ and $Y$ are independent random variables both uniformly distributed on $(0,1)$, then calculate the probability of $X+Y$.

\textbf{Solution:} since 
\begin{align*}
f(a) = g(a) =
\begin{cases}
1 & \mbox{0 < a < 1} \\
0 & otherwise
\end{cases}
\end{align*}

We obtain
\impo{
\begin{align*}
f_{X+Y}(a) =\int_0^1 f(a-y)g(y)dy =  \int_0^a f(a-y)dy 
\end{align*}
}

For $0\leq a \leq 1$, this yields

\begin{align*}
f_{X+Y}(a) = \int_0^a dy = a
\end{align*}

For $1<a<2$, we get
\begin{align*}
f_{X+Y}(a) = \int_{(a-1)}^1 dy = 2-a
\end{align*}

Hence, we have
\begin{align*}
f_{X+Y}(a) = 
\begin{cases}
a & \mbox{$0 \leq a \leq 1$} \\
2-a & \mbox{1 <a <2} \\
0 & \mbox{otherwise}
\end{cases}
\end{align*}

\paragraph{Sum of Independent Poisson Random Variables} Let $X$ and $Y$ be independent Poisson random variables with respective means $\lambda_1$ and $\lambda_2$. Then $X_1+X_2$ has a Poisson distribution with mean $\lambda_1 + \lambda_2$.

\todo{Read Page 55-56, Joint Probability Distribution of Functions of Random Variables }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Moment Generating Functions}
\paragraph{Definition} The \impo{moment generating function $\phi(t)$} of the random variable $X$ is defined for all values $t$ by
\begin{align*}
\phi(t) = E[e^{tX}] = 
\begin{cases}
\sum_x e^{tx} p(x) & \mbox{if X is discrete} \\
\sum_{-\infty}^\infty e^{tx} f(x) dx \mbox{if $X$ is continous}
\end{cases}
\end{align*}

We call $\phi(t)$ the moment generating function \impo{because all of the moments of $X$ can be obtained by successively differentiating $\phi(t)$}. For example
\begin{align*}
\phi'(t) = \frac{d}{dt} E[e^{tX}] = E[Xe^{tX}]
\end{align*}

Hence, $\phi'(0) = E[X]$

Similarly, $\phi''(t) = E[X^2 e^{tX}]$ and so $\phi''(0) = E[X^2]$.

In general, the $n$-th derivative of $\phi(t)$ evaluated at $t=0$ equals $E[X^n]$, that is
\begin{align*}
\phi^n(t) = E[X^n], n \geq 1
\end{align*}

\paragraph{Example-The Binomial Distribution with Parameters n and p} 
\begin{align*}
\phi(t) & = \sum^n_{k=0} \impo{e^{tk}} \binom{n}{k} p^k (1-p)^{n-k} \\
& = \sum^n_{k=0} \binom{n}{k} (pe^t)^k(1-p)^{n-k} \\
& =(pe^t+1-p)^n
\end{align*}

thus $\phi'(t) = n([e^t+1-p)^{n-1} pe^t$, and so $E[X] = \phi'(0) = np$.
and by calculate $\phi''(t)$, we get $E[X^2] = \phi''(0) = n(n-1)p^2 + np$. 


\paragraph{Example: The Normal Distribution with Parameters $\mu$ and $\sigma^2$} The moment generating function for a standard normal random variable is obtained as follows
\begin{align*}
E[e^{tZ}] & = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty e^{tx} e^{-x^2/2} dx \\
& = e^{t^2/2}
\end{align*}

If $Z$ is a standard normal, then $X = \sigma Z + \mu$ is normal with parameters $\mu$ and $\sigma^2$, therefore
\begin{align*}
\phi(t) = E[e^{tX}] & = E[e^{t(\sigma Z + \mu}] \\
& = e^{t\mu} E[e^{t \sigma Z}] \\
& = exp\{\frac{\sigma^2 t^2}{2} + \mu t\}
\end{align*}



\paragraph{Sum of Independent Random Variables} Moment generating function of the \impo{sum of independent random variables} is just the \impo{product} of the individual moment generating functions. 

To see this, suppose that $X$ and $Y$ are independent and have moment generating function $\phi_X(t)$ and $\phi_Y(t)$, respectively. Then $\phi_{X+Y}(t)$, the moment generating function of $X+Y$, is given by
\begin{align*}
\phi_{X+Y}(t) = E[e^{t(X+Y)}] = E[e^{tX} e^{tY}] = E[e^{tX}] E[e^{tY}] = \phi_X(t) \phi_Y(t)
\end{align*}


\paragraph{\impo{Laplace transform}} For a nonnegative random variable $X$, it is often convenient to define its \emph{Laplace transform} $g(t), t \geq 0$, by
\begin{align*}
g(t) = \phi(-t) = E[e^{-tX}] 
\end{align*}

This is, the Laplace transform evaluated at $t$ is just the moment generating function evaluated at $-t$. The \impo{advantage} of dealing with the Laplace transform, rather than the moment generating function, when the random variable is nonnegative is that if $X \geq 0$ and $t \geq 0$, then
\impo{
\begin{align*}
0 \leq e^{-tX} \leq 1
\end{align*}
}

That is, the Laplace transform is always between $0$ and $1$. As in the case of moment generating functions, it remains true that nonnegative random varaibles that have \impo{the same Laplace transform must also have the same distribution}.


\subsection{The Joint Distribution of the Sample Mean and Sample Variance from a Normal Distribution}
Let $X_1, \cdots, X_n$ be independent and identically distributed random variables, each with mean $\mu$ and variance $\sigma^2$. The random variable $S^2$ is defined by
\begin{align*}
S^2 = \sum^n_{i=1} \frac{(X_i - \bar{X})^2}{n-1}
\end{align*}
is called the \impo{sample variance} of the data. To compute $E[S^2]$, we use the identity
\begin{align*}
\sum_n{i=1} (X_i - \bar{X})^2 = \sum^n_{i=1} (X_i - \mu)^2 - n(\bar{X} - \mu)^2
\end{align*}

which is proven as follows
\begin{align*}
\sum^n_{i=1} (X_i - \bar{X}) & = \sum^n_{i=1} (X_i - \mu + \mu - \bar{X})^2 \\
& = \sum^n_{i=1} (X_i - \mu)^2 - n(\bar{X} - \mu)^2
\end{align*}

Using this identity, we have
\begin{align*}
E[(n-1)S^2] & = \sum^n_{i=1} E[(X_i - \mu)^2] - nE[(\bar{X} - \mu)^2] \\
& = n \sigma^2 - n Var(\bar{X}) \\
& = n \sigma^2 - n *\frac{\sigma^2}{n} \\
& = (n-1) \sigma^2
\end{align*}

Thus, we obtain the preceding that 
\begin{align*}
E[S^2] = \sigma^2
\end{align*}


\paragraph{Chi-square random variable} If $Z_1, \cdots, Z_n$ are independent standard normal random variables, then the random variable $\sum^n_{i=1} (Z_i)^2$ is said to be a \impo{chi-squared random variable} with \impo{n degrees of freedom}.

\todo{review chi-square}

\begin{proposition}
If $X_1, \cdots, X_n$ are independent and identically distributed normal random variables with mean $\mu$ and variance $\sigma^2$, then the sample mean $\bar{X}$ and the sample variance $S^2$ are independent. $\bar{X}$ is a normal random variable with \impo{mean $\mu$ and variance $\sigma^2/n$}; $\frac{(n-1)\S^2}{\sigma^2}$ is a chi-squared random variable with $(n-1)$ degrees of freedom.
\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Distribution of the Number of Events that Occur}

\subsection{Define $A$, $B$, $B^c$}
Consider arbitrary events $A_1, \cdots, A_n$, and let $X$ denote the number of these events that occur. We will determine the probability mass function of $X$. To begin with, $1 \leq k \leq n$, let 
\begin{align*}
S_k = \sum_{i1 < ... < ik} P(A_{i1}...A_{ik})
\end{align*}
equal the sum of the probabilities of all the $\binom{n}{k}$ intersections of $k$ distinct events, and note that the \impo{inclusion-exclusion} identity states that
\begin{align*}
P(X>0) = P(\cup^n_{i=1} A_i) = S_1 - S_2 + S_3 - \cdots + (-1)^{n+1} S_n
\end{align*}

Now, fix $k$ of the n events, say $A_{i1}, \cdots, A_{ik}$, and let 
\begin{align*}
A = \cap^k_{j=1} A_{ij}
\end{align*}

be the event that $k$ of these events occur. Also let
\begin{align*}
B = \cap_{j \not\in {i_1, \cdots, i_k} }A^c_j 
\end{align*}
be the events that none of the other $(n-k)$ events occur. Consequently, AB is the event that $A_{i1}, \cdots, A_{ik}$ are the only events to occur. Because
\begin{align*}
A = AB \cup AB^c
\end{align*}

Because $B^c$ is at least one of the events $A_j, j \neq {i_1, \cdots, i_k}$, occur, we see that 
\begin{align*}
B^c = \cup_{j \not\in {i_1, \cdots, i_k}} A_j
\end{align*}

Thus
\begin{align*}
P(AB^c) = P(A \cup_{j \not\in \{i_1, \cdots, i_k\}} A_j) = P(\cup_{j \not\in \{i_1, \cdots, i_k\}} AA_j)
\end{align*}

\subsection{Calculate $P(X=k)$}
Applying the \impo{inclusion-exclusion} identity gives
\begin{align*}
P(AB^c) = \sum_{j \not\in \{i_1, \cdots, i_k\}} P(AA_j) - \sum_{j_1 < j_2 \not\in \{i_1, \cdots, i_k\}} P(AA_{j_1}A_{j_2}) \\
+ \sum_{j_1 < j_2 < j_3 \not\in \{i_1, \cdots, i_k\}} P(AA_{j_1}A_{j_2}A_{j_3})
\end{align*}

using that $A = \cap^k_{j=1} A_{ij}$, the preceding shows that the probability that the $k$ events $A_{i1}, \cdots, A_{ik}$ are the only events to occur is
\begin{align*}
P(A) - P(AB^c) & = P(A_{i1}, \cdots, A_{ik}) - \sum_{j \not\in \{i_1, \cdots, i_k\}} P(A_{i1}, \cdots, A_{ik}A_j)\\
& +  \sum_{j_1 < j_2 \not\in \{i_1, \cdots, i_k\}} P(A_{i1}, \cdots, A_{ik}A_{j_1}A_{j_2})  - \sum_{j_1 < j_2 < j_3 \not\in \{i_1, \cdots, i_k\}} P(A_{i1}, \cdots, A_{ik}A_{j_1}A_{j_2}A_{j_3}) + \cdots
\end{align*}

Summing the preceding over all sets of $k$ distinct indices yields
\begin{align*}
P(X=k) & = \sum_{i_1 < \cdots < i_k} P(A_{i1}, \cdots, A_{ik}) - \sum_{i_1 < \cdots < i_k} \sum_{j \not\in \{i_1, \cdots, i_k\}} P(A_{i1}, \cdots, A_{ik}A_j)\\
& +  \sum_{i_1 < \cdots < i_k} \sum_{j_1 < j_2 \not\in \{i_1, \cdots, i_k\}} P(A_{i1}, \cdots, A_{ik}A_{j_1}A_{j_2})  - \cdots
\end{align*}

First, note that 
\begin{align*}
\sum_{i_1, \cdots, i_k} P(A_{i1} \cdots A_{ik}) = S_k
\end{align*}

The probability of every intersection of $k+1$ distinct events $A_{m1} \cdots A_{m{k+1}}$ will appear $\binom{k+1}{k}$ times in this multiple summation. This is so because each of $k$ of of its indices to play the role of $i_1, \cdots, i_k$ and the other to play the role of $j$ results in the addition of therm $P(A_{m1} \cdots A_{mk+1})$, hence
\begin{align*}
\sum_{i_1 < \cdots i_k} \sum_{j \not\in \{i_1, \cdots, i_k\}} P(A_{i1} \cdots A_{ik} A_j) & = \binom{k+1}{k} \sum_{m_1 < \cdots < mk+1} P(A_{m1} < \cdots < A_{mk+1}) \\
& = \binom{k+1}{k} S_{k+1}
\end{align*}

Repeating this argument for the rest of the multiple summations yields the result
\begin{align*}
P(X=k) = S_k - \binom{k+1}{k} S_{k+1} + \binom{k+2}{k} S_{k+2} - \cdots + (-1)\binom{n}{k}S_n
\end{align*}

The preceding can be written as 
\begin{align*}
P(X=k) = \sum^n_{j=k} (-1)^{k+j} \binom{j}{k} S_j
\end{align*}


\subsection{Calculate $P(X \geq k)$} We will prove
\begin{align*}
P(X \geq k) = \sum^n_{j=k} (-1)^{k+j} \binom{j-1}{k-1} S_j
\end{align*}

This proof uses a \impo{backwards mathematical induction} that starts with $k=n$. Now when $k=n$ the proceeding identity states that 
\begin{align*}
P(X=n) = S_n
\end{align*}
which is true. So assume that 
\begin{align*}
P(X \geq k+1) \sum^n_{j=k+1} (-1)^{k+1+j} \binom{j-1}{k}S_j
\end{align*}

But then 
\begin{align*}
P(X \geq k) & = P(X=k) + P(X \geq k+1) \\
& = \sum^n_{j=k} (-1)^{k+j} \binom{j}{k} S_j + \sum^n_{j=k+1} (-1)^{k+1+j} \binom{j-1}{k} S_j \\
& = \sum^n_{j=k} (-1)^{k+j} \binom{j-1}{k-1} S_j
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limit Theorems}
\paragraph{Markov's Inequality} If $X$ is a random variable that takes only nonnegative values, then for any value $a > 0$
\begin{align*}
P\{X \geq a\} \leq \frac{E[X]}{a}
\end{align*}

\begin{proof}
We give a proof for the case where $X$ is continuous with density $f$.
\begin{align*}
E[X] & = \int_0^{\infty} xf(x)dx \\
& = \impo{\int_0^a} x(f)dx + \impo{\int_a^{\infty}} xf(x)dx \\
& \geq \int_a^{\infty} xf(x)dx \\
& \geq \int_a^{\infty} \impo{a}f(x)dx \\
& = a \int_a^{\infty} f(x) dx \\
& = a P{X \geq a}
\end{align*}
\end{proof}


\paragraph{Chebyshev's Inequality} If $X$ is a random variable with mean $\mu$ and variance $\sigma^2$, for any value $k >0$,
\begin{align*}
P\{|X-\mu| \geq k\} \leq \frac{\sigma^2}{k}
\end{align*}

\begin{proof}
Since $(X-\mu)^2$ is a nonnegative random variable, we can \impo{ apply Markov's inequality}(with $a = k^2$) to obtain
\begin{align*}
P\{ (X-\mu)^2 \geq k^2\} \leq \frac{E[(X-\mu)^2]}{k^2}
\end{align*}

But since $(X-\mu)^2 \geq k^2$ if an only if $|X-\mu| \geq k$, the preceding is equivalent to 
\begin{align*}
P\{\|X-\mu| \geq k\} \leq \frac{E[(X-\mu)^2]}{k^2} = \frac{\sigma^2}{k^2}
\end{align*}
\end{proof}

\paragraph{Importance} The importance of Markov's and Chebyshev's inequalities is that they enable us to \impo{derive bounds} on probabilities when only the mean, or both the mean and the variance, of the probability distribution are known. Of course, if the actual distribution were known, then the desired probabilities can be exactly computed, and we would not need to resort bounds.


\paragraph{Strong Law of Large Numbers} It states that the \impo{average} of a sequence of independent random variables having the same distribution will, with probability $1$, \impo{converge to the mean} of that distribution.


\paragraph{Central Limit Theorem} Let $X_1, X_2, \cdots$ be a sequence of independent, identically distributed random variables, each with mean $\mu$ and variance $\sigma^2$. Then the distribution of 
\begin{align*}
\frac{X_1 + X_2 + \cdots + X_n - n\mu}{\sigma \sqrt{n}}
\end{align*}

tends to the standard normal as $n \to \infty$. That is
\begin{align*}
P\{\frac{X_1 + X_2 + \cdots + X_n - n\mu}{\sigma \sqrt{n}}\} 
\end{align*}

Note that this theorem holds for \impo{any distribution} of the $X_i$s. Herein lies its power.

If $X$ is binomially distributed with parameters $n$ and $p$, then $X$ has the same distribution as the sum of $n$ independent Bernoulli random variables, each with parameter $p$. Hence the distribution of 
\begin{align*}
\frac{X-E[X]}{\sqrt{Var(xï¼‰}} =  \frac{X-np}{\sqrt{np(1-p)}}
\end{align*}

approach the standard normal distribution as $n$ approaches $\infty$. The normal distribution will, in general, be quite good for values of $n$ satisfying $np(1-p) \geq 10$.


\paragraph{Example-Normal Approximation to the Binomial} See page 74.

\paragraph{Example} The lifetime of a special type of battery is a random varaible which mean $40$ hours and standard deviation $20$ hours. A battery is used until it fails, at which point it is replaced by a new one. Assuming a stockpile of $25$ such batteries, the lifetime of which are independent, approximate the probability that over $1100$ hours of use can be obtained. 

\textbf{Solution:} If we let $X_i$ denote the lifetime of the $i$-th battery to be put int use, then we desire $p = \{X_1 + \cdots + X_{25} > 10000\}$, which is approximated as follows
\begin{align*}
p & = P\{\frac{X-1 + \cdots + X_{25}-20*25}{20\impo{\sqrt{25}}} > \frac{1100-1000}{20\sqrt{25}}\} \\
& \approx P\{N(0,1) > 1\} \\
& = 1- \phi(1) \\
& \approx 0.1587
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stochastic Process}
\paragraph{Definition} \emph{A stochastic process} $\{X(t), t \in T\}$ is a collection of random variables. That is, for each $t \in T$, $X(t)$ is a random variable. The index $t$ is often interpreted as \impo{time} and, as a result, we refer $X(t)$ as the state of the process at time $t$.

A stochastic process is a family of random variables that \impo{describes the evolution through time of some (physical) process}.

\paragraph{Example} Consider a particle that moves along a set of $m=1$ nodes, labeled $0,1, \cdots, m$, that are arranged around a circle. Suppose now that the particle starts at $0$ and continues to move around according to the preceding rules until all the nodes $1,2, \cdots, m$ have been visited. What is the probability that node $i, i=1,2,\cdots, m$ is the last one visited?

\textbf{Solution} We consider two neighbors $i-1$ and $i+1$ of $i$. If it is the first time at $i-1$, then $i$ will be the last to be visited only if $i+1$ has been visited before. 

The probability that a particle at node $i-1$ will visited $i+1$ before $i$ is just the probability that a particle will progress \impo{$m-1$} steps in a specified direction before progressing one step in the other direction. That is,  it is equal to the probability that a gambler who starts with one unit, and wins one when a fiar coin turns up heads and loses one when it turns up tails, will \impo{have his fortune go up by $m-1$ before he goes broke
} \question{why???}. Hence, because \impo{the preceding implies that the probability that node $i$ is the last node visited is the same for all $i$}, and because this probabilities must sum up to $1$, we obtain
\begin{align*}
p\{\mbox{i is the last node visited}\} = \frac{1}{m}, i = 1, 2, \cdots, m
\end{align*}


\paragraph{Remark} A gambler who is equally likely to either win or lose one unit on each gamble will be down $n$ before being up $1$ with probability $\frac{1}{n+1}$; or equivalently
\begin{align*}
p\{\mbox{gambler is up to 1 before being down $n$}\} = \frac{n}{n+1}
\end{align*}

Suppose now we ant the probability that the gambler is up $2$ before being down $n$. Upon conditioning on whether he reaches up $1$ before down $n$, we obtain
\begin{align*}
& p\{\mbox{gambler is up $2$ before being down $n$}\}  \\
& = p\{\mbox{up 2 before down n | up 1 before down n|}\}*\frac{n}{n+1} \\
& = p\{\mbox{up 1 before down n+1}\} \frac{n}{n+1} \\
& = \frac{n+1}{n+2} * \frac{n}{n+1} \\
& = \frac{n}{n+2}
\end{align*}

Repeating this, we get 
\begin{align*}
p\{ \mbox{gambler is up $k$ before downing $n$}\} = \frac{n}{n+k}
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%