%\documentclass[10pt,conference]{IEEEtran}

\documentclass[10 pt,final]{article}

\usepackage{amssymb} \usepackage{amsmath} \usepackage{amsthm} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{url} \usepackage[margin=1in]{geometry}

\usepackage{subfigure}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{definition}{Definition} \newtheorem{assumption}{Assumption} \newtheorem{example}{Example}
\newtheorem{observation}[theorem]{Observation}
%\newtheorem{theorem}{Theorem} \newtheorem{definition}{Definition} \newtheorem{remark}{Remark} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{fact}{Fact} \newtheorem{invariant}{Invariant}

\usepackage{color}
\newcounter{todocounter}
\newcommand{\todo}[1]{\stepcounter{todocounter}\textcolor{red}{to-do\#\arabic{todocounter}: #1}}
\newcommand{\impo}[1]{{\color{magenta} #1}}
\newcommand{\question}[1]{{\color{blue} #1}}



\usepackage{graphicx}
\graphicspath{{./Figures/}}

\title{Note of: An Introduction to Information Theory and Entropy}


\begin{document}



%\author{Huangxin Wang\thanks{Department of Computer Science, George Mason University. Fairfax, VA 22030. Email: \textsf{hwang14@gmu.edu}}}
\date{}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Entropy}
\paragraph{Information Theory} Measures related to how surprising or unexpected an observation or event is. This approach has been described as \emph{information theory}.

\paragraph{Definition} We have defined \emph{information} strictly in terms of the probabilities of events. Therefore, let us suppose we have a set of probabilities(a probability distribution) $P=\{p1_,p_2, \cdots, p_n\}$. We define the entropy of the distribution $P$ by
\begin{align*}
H(P) = \sum^n_{i=1} p_i*\log{(1/p_i)}
\end{align*}

We can think about this as the expected value. In other words, the entropy of a probability distribution is just the expected value of the information of the distribution. 

\paragraph{\impo{The Gibbs Inequality}}
First, note that the function $\ln(x)$ has derivative $1/x$. From this, we find that the tangent to $\ln(x)$ at $x=1$ is the line $y=x-1$. Further, since $\ln(x)$ is concave down, we have, for $x >0$, that 
\begin{align*}
ln(x) \leq x -1
\end{align*}
with equality only when $x=1$. Now given two probability distributions,\\
$P=\{p_1, p_2, \cdots, p_n\} and $ \\
$Q=\{q_1, q-2, \cdots, q_n\},$ where $p_i, q_i \geq 0$, \\
and $\sum_i p_i = \sum_i q_i =1$, we have

\impo{
\begin{align*}
\sum^n_{i=1} p_i \ln(q_i/p_i) \leq \sum^n_{i=1} p_i*(q_i/p_i -1) = \sum^n_{i=1} (q_i-p_i) = \sum^n_{i=1} q_i - \sum^n_{i=}p_i = 1 - 1 =0
\end{align*}
with equality only when $p_i = q_i$ for all $i$. }

\paragraph{Application of Gibbs Inequality} We can use the Gibbs inequality to find the probability distribution which maximize the entropy function. Suppose $P=\{p_1, p_2, \cdots, p_n\}$ is a probability distribution. We have
\begin{align*}
H(P) - log(n) & = \sum^n_{i=1} p_i *\log{1/p_i} - log(n) \\
& = \sum^n_{i=1} p_i \log(1/p_i) - log(n) \sum^n_{i=1} p_i \\
& = \sum^n_{i=1} p_i \log(1/p_i) - \sum^n_{i=1} p_i \log{n} \\
& = \sum^n_{i=1} p_i \log{\frac{1/n}{p_i}} \\
& \leq 0
\end{align*}
with equality only when $p_i = 1/n$ for all $i$. The last step is the application of Gibbs inequality.

That this means is that 
\impo{
\begin{align*}
0 \leq H(n) \leq \log(n)
\end{align*}
}
That is, the maximum entropy is achieved when all the events are equally likely.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Shannon's Communication Theory}
\subsection{Introduction}
In his classic 1948 papers, Claude Shannon laid the foundations for contemporary information, coding , and communication theory. He developed a general model for communication systems, and a set of theoretical tools for analyzing such systems. 

His \impo{basic model} consists of three parts: a sender(or source), a channel, and a receiver(or sink). His general model also includes encoding and decoding elements, and noise within the channel.

\paragraph{Model} In Shannon's discrete model, it is assumed that the source provides a stream of symbols selected from a finite alphabet $A=\{a_1, a_2, \cdots, a_n\}$, which are then encoded. The code is sent through the channel(and possibly disturbed by noise). At the other end of the channel, the receiver will decode, and derive information from the sequence of symbols. 

Given a source of symbols, and a channel with noise(in particular, a probability model for these elements), we can talk about the capacity of the channel. The general model Shannon worked with involved two sets of symbols, the input symbols and the output symbols. Let us say the two sets of symbols are
\begin{align*}
A = \{a_1, s_2, \cdots, a_n\} and  \\
B = \{b-1, b_2, \cdots, b_m\}.
\end{align*}

Note that we do not necessarily assume the same number of symbols in the two sets. Given the noise in the channel, when symbol $b_j$ comes out of the channel, we can not be sure which $a_i$ was put in. The channel is characterized by the set of probabilities $\{P(a_i|b_j)\}$.

\paragraph{Mutual Information} We consider information we get from observing a symbol $b_j$. Given a probability model of the source, we have an priori estimate $P(a_i)$ that symbol $a_i$ will be sent next. Upon observing $b_j$, we can revise our estimate to $P(a_i|b_j)$. The change in our information(\impo{the mutual information}) will be given by:
\begin{align*}
I(a_i;b_j) & = log(\frac{1}{P(a_i)} - log(\frac{1}{P(a_i|b_j)}) \\
& = log(\frac{P(a_i|b_j)}{P(a_i)}
\end{align*} 

We have the properties:
\begin{align*}
I(a_i; b_j) = I(b_j; a_i) \\
I(a_i; b_j) = log(P(a_i|b_j)) + I(a_i) \\
I(a_i, b_j) \leq I(a_i)
\end{align*}

If $a_i$ and $b_j$ are independent (i.e., if $P(a_i,b_j) = P(a_i) * P(b_j)$), then 
\begin{align*}
I(a_i, b_j) = 0
\end{align*}

\paragraph{Average the mutual information over all the symbols}
\begin{align*}
I(A; b_j) & = \sum_i P(a_i|b_j)*I(a_i;b_j) \\
& =\sum_i P(a_i |b_j) * log(\frac{P(a_i|b_j)}{P(a_i)})
\end{align*}

\paragraph{Channel Capacity C} We define the Channel Capacity to be 
\begin{align*}
C = \max_{P(a)} I(A;B)
\end{align*}

\question{Property} We have the nice property that if we are using the channel at its capacity, then for each of the $a_i$
\begin{align*}
I(a_i;B) = C
\end{align*}
and thus, \impo{we can maximize channel use by maximizing the use for each symbol independently}.

\paragraph{Shannon Main Theorem} For any channel, there exist ways of \impo{encoding input symbols} such that we can simultaneously utilize the channel as closely as we wish to the capacity, and at the same time have an error rate as close to zero as we wish.

This is actually quite a remarkable theorem. We might naively guess that in order to minimize the error rate, we would have to use more o the channel capacity for error detection/correction, and less for actual transmission of information. \impo{Shannon showed that it is possible to keep error rates low and still use the channel for information transmission at(or near) its capacity}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Some other measurements}
\paragraph{Moment}
There have been various approaches to expanding on the idea of entropy as a measure of complexity. One useful generalization of entropy was developed by the Hungarian methematician A. Renyi. His method involves looking at \question{the moments} of order $q$ of a probability distribution $\{p_i\}$
\begin{align*}
S_q = \frac{1}{q-1}log \sum_i (p_i)^q
\end{align*}


\question{how?}

If we take the limit as $q \to 1$, we get:
\begin{align*}
S_1 = \sum_i p_i log(1/p_i)
\end{align*}
the entropy we have previously defined. We can then think of \impo{$S_q$ as a generalized entropy for any real number $q$.}

\paragraph{Dimension} Expanding on these generalized entropies, we can then define a generalized dimension associated with a data set. If we imaging the data set to be distributed among bins of diameter $r$, we can let $p_i$ be the probability that a data item falls in the $i$-th bin(estimated by counting the data elements in the bin, and dividing by the total number of items). We can then, for each $q$, define a dimension

\question{why make such definition?}
\begin{align*}
D_q = \lim_{r \to 0} \frac{1}{q-1} \frac{log \sum_i (p_i)^q}{log(r)}
\end{align*}

\impo{why do we call this a generalized dimension?} Consider $D_0$. First, we sill adopt the convention that $(p_i)^0$ when $p_i = 0$. Also, let $N_r$ be the number of non-empty bins(i.e., the number of bins of diameter $r$ it takes to cover the data set).

Then we have:
\begin{align*}
D_0 = \lim_{r \to 0} \frac{log \sum_i (p_i)^0}{log(1/r)} = \lim_{r \to 0} \frac{log(N_r)}{log(1/r)}
\end{align*}

Thus, $D_0$ is the hausdorff dimension $D$, which is frequently in the literature called the \impo{fractal dimension} of the set.

\question{Need Explain}

\paragraph{Example 1} Consider the unit interval $[0,1]$. Let $r_k = 1/(2)^k$. Then $N_{rk} = 2^{2k}$, and
\begin{align*}
D_0 = \lim{k \to \infty} \frac{log(2^{k})}{log(2^k)} = 1
\end{align*}

\paragraph{Example 2} Consider the unit square $[0,1]*[0,1]$. Again, let $r_k = 1/2^k$. Then $N_{rk} = 2^{2k}$, and
\begin{align*}
D_0 = \lim_{k \to \infty} \frac{log(2^{2k})}{log(2^k)} = 2
\end{align*}

\paragraph{Example 3} Consider the \impo{Cantor Set} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Examples using Bayes' Theorem}
\end{document}