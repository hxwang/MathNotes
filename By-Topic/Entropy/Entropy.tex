%\documentclass[10pt,conference]{IEEEtran}

\documentclass[10 pt,final]{article}

\usepackage{amssymb} \usepackage{amsmath} \usepackage{amsthm} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{url} \usepackage[margin=1in]{geometry}

\usepackage{subfigure}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{definition}{Definition} \newtheorem{assumption}{Assumption} \newtheorem{example}{Example}
\newtheorem{observation}[theorem]{Observation}
%\newtheorem{theorem}{Theorem} \newtheorem{definition}{Definition} \newtheorem{remark}{Remark} \newtheorem{lemma}{Lemma} \newtheorem{corollary}{Corollary} \newtheorem{fact}{Fact} \newtheorem{invariant}{Invariant}

\usepackage{color}
\newcounter{todocounter}
\newcommand{\todo}[1]{\stepcounter{todocounter}\textcolor{red}{to-do\#\arabic{todocounter}: #1}}
\newcommand{\impo}[1]{{\color{magenta} #1}}
\newcommand{\question}[1]{{\color{blue} #1}}



\usepackage{graphicx}
\graphicspath{{./Figures/}}

\title{Note of: An Introduction to Information Theory and Entropy}


\begin{document}



%\author{Huangxin Wang\thanks{Department of Computer Science, George Mason University. Fairfax, VA 22030. Email: \textsf{hwang14@gmu.edu}}}
\date{}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Notes for Materials "An introduction to information theory and entropy", by Tom Carter.

\section{Entropy}
\paragraph{Information Theory} Measures related to how surprising or unexpected an observation or event is. This approach has been described as \emph{information theory}.

\paragraph{Definition} We have defined \emph{information} strictly in terms of the probabilities of events. Therefore, let us suppose we have a set of probabilities(a probability distribution) $P=\{p1_,p_2, \cdots, p_n\}$. We define the entropy of the distribution $P$ by
\begin{align*}
H(P) = \sum^n_{i=1} p_i*\log{(1/p_i)}
\end{align*}

We can think about this as the expected value. In other words, the entropy of a probability distribution is just the expected value of the information of the distribution. 

\paragraph{\impo{The Gibbs Inequality}}
First, note that the function $\ln(x)$ has derivative $1/x$. From this, we find that the tangent to $\ln(x)$ at $x=1$ is the line $y=x-1$. Further, since $\ln(x)$ is concave down, we have, for $x >0$, that 
\begin{align*}
ln(x) \leq x -1
\end{align*}
with equality only when $x=1$. Now given two probability distributions,\\
$P=\{p_1, p_2, \cdots, p_n\} and $ \\
$Q=\{q_1, q-2, \cdots, q_n\},$ where $p_i, q_i \geq 0$, \\
and $\sum_i p_i = \sum_i q_i =1$, we have

\impo{
\begin{align*}
\sum^n_{i=1} p_i \ln(q_i/p_i) \leq \sum^n_{i=1} p_i*(q_i/p_i -1) = \sum^n_{i=1} (q_i-p_i) = \sum^n_{i=1} q_i - \sum^n_{i=}p_i = 1 - 1 =0
\end{align*}
with equality only when $p_i = q_i$ for all $i$. }

\paragraph{Application of Gibbs Inequality} We can use the Gibbs inequality to find the probability distribution which maximize the entropy function. Suppose $P=\{p_1, p_2, \cdots, p_n\}$ is a probability distribution. We have
\begin{align*}
H(P) - log(n) & = \sum^n_{i=1} p_i *\log{1/p_i} - log(n) \\
& = \sum^n_{i=1} p_i \log(1/p_i) - log(n) \sum^n_{i=1} p_i \\
& = \sum^n_{i=1} p_i \log(1/p_i) - \sum^n_{i=1} p_i \log{n} \\
& = \sum^n_{i=1} p_i \log{\frac{1/n}{p_i}} \\
& \leq 0
\end{align*}
with equality only when $p_i = 1/n$ for all $i$. The last step is the application of Gibbs inequality.

That this means is that 
\impo{
\begin{align*}
0 \leq H(n) \leq \log(n)
\end{align*}
}
That is, the maximum entropy is achieved when all the events are equally likely.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Shannon's Communication Theory}
\subsection{Introduction}
In his classic 1948 papers, Claude Shannon laid the foundations for contemporary information, coding , and communication theory. He developed a general model for communication systems, and a set of theoretical tools for analyzing such systems. 

His \impo{basic model} consists of three parts: a sender(or source), a channel, and a receiver(or sink). His general model also includes encoding and decoding elements, and noise within the channel.

\paragraph{Model} In Shannon's discrete model, it is assumed that the source provides a stream of symbols selected from a finite alphabet $A=\{a_1, a_2, \cdots, a_n\}$, which are then encoded. The code is sent through the channel(and possibly disturbed by noise). At the other end of the channel, the receiver will decode, and derive information from the sequence of symbols. 

Given a source of symbols, and a channel with noise(in particular, a probability model for these elements), we can talk about the capacity of the channel. The general model Shannon worked with involved two sets of symbols, the input symbols and the output symbols. Let us say the two sets of symbols are
\begin{align*}
A = \{a_1, s_2, \cdots, a_n\} and  \\
B = \{b-1, b_2, \cdots, b_m\}.
\end{align*}

Note that we do not necessarily assume the same number of symbols in the two sets. Given the noise in the channel, when symbol $b_j$ comes out of the channel, we can not be sure which $a_i$ was put in. The channel is characterized by the set of probabilities $\{P(a_i|b_j)\}$.

\paragraph{Mutual Information} We consider information we get from observing a symbol $b_j$. Given a probability model of the source, we have an priori estimate $P(a_i)$ that symbol $a_i$ will be sent next. Upon observing $b_j$, we can revise our estimate to $P(a_i|b_j)$. The change in our information(\impo{the mutual information}) will be given by:
\begin{align*}
I(a_i;b_j) & = log(\frac{1}{P(a_i)} - log(\frac{1}{P(a_i|b_j)}) \\
& = log(\frac{P(a_i|b_j)}{P(a_i)}
\end{align*} 

We have the properties:
\begin{align*}
I(a_i; b_j) = I(b_j; a_i) \\
I(a_i; b_j) = log(P(a_i|b_j)) + I(a_i) \\
I(a_i, b_j) \leq I(a_i)
\end{align*}

If $a_i$ and $b_j$ are independent (i.e., if $P(a_i,b_j) = P(a_i) * P(b_j)$), then 
\begin{align*}
I(a_i, b_j) = 0
\end{align*}

\paragraph{Average the mutual information over all the symbols}
\begin{align*}
I(A; b_j) & = \sum_i P(a_i|b_j)*I(a_i;b_j) \\
& =\sum_i P(a_i |b_j) * log(\frac{P(a_i|b_j)}{P(a_i)})
\end{align*}

\paragraph{Channel Capacity C} We define the Channel Capacity to be 
\begin{align*}
C = \max_{P(a)} I(A;B)
\end{align*}

\question{Property} We have the nice property that if we are using the channel at its capacity, then for each of the $a_i$
\begin{align*}
I(a_i;B) = C
\end{align*}
and thus, \impo{we can maximize channel use by maximizing the use for each symbol independently}.

\paragraph{Shannon Main Theorem} For any channel, there exist ways of \impo{encoding input symbols} such that we can simultaneously utilize the channel as closely as we wish to the capacity, and at the same time have an error rate as close to zero as we wish.

This is actually quite a remarkable theorem. We might naively guess that in order to minimize the error rate, we would have to use more o the channel capacity for error detection/correction, and less for actual transmission of information. \impo{Shannon showed that it is possible to keep error rates low and still use the channel for information transmission at(or near) its capacity}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Some other measurements}
\paragraph{Moment}
There have been various approaches to expanding on the idea of entropy as a measure of complexity. One useful generalization of entropy was developed by the Hungarian methematician A. Renyi. His method involves looking at \question{the moments} of order $q$ of a probability distribution $\{p_i\}$
\begin{align*}
S_q = \frac{1}{q-1}log \sum_i (p_i)^q
\end{align*}


\question{how?}

If we take the limit as $q \to 1$, we get:
\begin{align*}
S_1 = \sum_i p_i log(1/p_i)
\end{align*}
the entropy we have previously defined. We can then think of \impo{$S_q$ as a generalized entropy for any real number $q$.}

\paragraph{Dimension} Expanding on these generalized entropies, we can then define a generalized dimension associated with a data set. If we imaging the data set to be distributed among bins of diameter $r$, we can let $p_i$ be the probability that a data item falls in the $i$-th bin(estimated by counting the data elements in the bin, and dividing by the total number of items). We can then, for each $q$, define a dimension

\question{why make such definition?}
\begin{align*}
D_q = \lim_{r \to 0} \frac{1}{q-1} \frac{log \sum_i (p_i)^q}{log(r)}
\end{align*}

\impo{why do we call this a generalized dimension?} Consider $D_0$. First, we sill adopt the convention that $(p_i)^0$ when $p_i = 0$. Also, let $N_r$ be the number of non-empty bins(i.e., the number of bins of diameter $r$ it takes to cover the data set).

Then we have:
\begin{align*}
D_0 = \lim_{r \to 0} \frac{log \sum_i (p_i)^0}{log(1/r)} = \lim_{r \to 0} \frac{log(N_r)}{log(1/r)}
\end{align*}

Thus, $D_0$ is the hausdorff dimension $D$, which is frequently in the literature called the \impo{fractal dimension} of the set.

\question{Need Explain}

\paragraph{Example 1} Consider the unit interval $[0,1]$. Let $r_k = 1/(2)^k$. Then $N_{rk} = 2^{2k}$, and
\begin{align*}
D_0 = \lim{k \to \infty} \frac{log(2^{k})}{log(2^k)} = 1
\end{align*}

\paragraph{Example 2} Consider the unit square $[0,1]*[0,1]$. Again, let $r_k = 1/2^k$. Then $N_{rk} = 2^{2k}$, and
\begin{align*}
D_0 = \lim_{k \to \infty} \frac{log(2^{2k})}{log(2^k)} = 2
\end{align*}

\paragraph{Example 3} Consider the \impo{Cantor Set} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analog Channels}
\paragraph{Problem Model} Suppose we have a signalling system using \impo{band-limited} signals(i.e., the frequencies of the transmissions are restricted to lie within some specified range). Let us call the bandwidth $W$. Let us further assume we are transmitting signals of duration $T$. In order to reconstruct a given signal, we will need $2WT$\question{why??} samples of the signal. Thus, if we are sending continuous signals, each signal can be represented by $2WT$ numbers $x_i$, taken at equal intervals.

\question{why define energy in such form??}
We are associated each signal an energy, given by
\begin{align*}
E = \frac{1}{2W} \sum^{2WT}_{i=1} (x_i)^2
\end{align*}

The \impo{distance} of the signal(from the original) will be
\begin{align*}
r = (\sum (x_i)^2)^{1/2} = (2WE)^{1/2}
\end{align*}

We can define the \impo{signal power} to be the average energy
\begin{align*}
S = \frac{E}{T}
\end{align*}

Then the \impo{radius} of the sphere of transmitted signals will be
\begin{align*}
r = (2WST)^{1/2}
\end{align*}

Each signal will be disturbed by the noise in the channel. If we measure the power of the noise $N$ added by the channel, the disturbed signal will lie in a sphere around the original signal of radius $(2WNT)^{1/2}$.

Thus the original sphere must be \impo{enlarged} to a larger radius to enclose the disturbed signals. The new radius will be
\begin{align*}
r = (2WT(S+N))^{1/2}
\end{align*}

In order to use the channel effectively and minimize error(misreading of signals.), we will want to put the signals in the sphere, \impo{and separate them as much as possible}(and have the distance between the signals \impo{at least twice} what the noise contributes...). We thus want to divide the sphere up into sub-spheres of radius = $(2WNT)^{1/2}$. From this, we can get an upper bound on the number $M$ of possible messages that we can reliably distinguish. We can use the formula for \impo{the volume of an $n$-dimensional sphere}
\begin{align*}
V(r,n) = \frac{\pi^{n/2} r^n}{\Gamma(n/2+1)}
\end{align*}

We have the bound
\begin{align*}
M \leq \frac{\pi^{WT} (2WT(S+N))^{WT}}{\Gamma(WT+1)} \frac{\Gamma(WT+1)}{\pi^{TW}(2WTN)^{WT}}
\end{align*}

The information sent is \impo{the log of the number of messages sent}(assuming they are equally likely), and hence
\begin{align*}
I = log(M) = WT*log(1+S/N)
\end{align*}
We thus have the usual signal/noise formula for channel capacity.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Maximum Entropy Principle}
\paragraph{Model} Suppose we have a system for which we can measure certain macroscopic characteristics. Suppose further that the system is made up of many microscopic elements, and the system is free to vary among various states. 
\paragraph{Conclusion} With probability essentially equal to $1$, the system will be observed in states with maximum entropy.

We will then sometimes be able to gain understanding of the system by applying a \impo{maximum information entropy principle(MEP)}, and using Lagrange multipliers, derive formulae for aspects of the system.

Suppose we have a set of macroscopic measurable characteristics $f_k, k=1,2, \cdots, M$(which we can think of as constraints on the system), which we assume are related to microscopic characteristics via
\begin{align*}
\sum_i p_i *(f_i)^{(k)} = f_k
\end{align*}

Of course, we also have the constraints
\begin{align*}
p_i \geq 0 \\
\sum_i p_i = 1
\end{align*}

\question{(how to use Lagrange multipliers??}
We want to maximize the entropy $\sum_i p_ilog(1/p_i)$, subject to these constraints. Using \impo{Lagrange multipliers} $\lambda_k$(one for each constraint), we have he general solution:
\begin{align*}
p_i = exp(-\lambda - \sum_k \lambda_k (f_i)^{(k)}
\end{align*}

If we define $Z$, called the partition function, by
\begin{align*}
Z(\lambda_1, \cdots, \lambda_M) = \sum_i exp(-\sum_k \lambda_k (f_i)^{(k)})
\end{align*}

then we have $e^\lambda = Z$, or $\lambda = ln(z)$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application: A Boltzmann Economy}
\paragraph{Model} Suppose there is a fixed amount of money($M$ dollars), and a fixed number of agents($N$) in the economy. Suppose that during each time step, each agent randomly selects another agent and transfers one dollar to the selected agent. An agent having no money doesn't go in debt. What will the long term(stable) distribution of money be? We can image that every agent starts with approximately the same amount of money, although in the long run, the starting distribution shouldn't matter.

\paragraph{Analysis} For this model, we are interested in looking at the distribution of money in the economy, so we are looking at the probabilities $\{p_i\}$ that an agent has the amount of money $i$. We are hoping to develop a model for the collection $\{p_i\}$.

If we let $n_i$ be the number of agents who have $i$ dollars, we have two constraints
\begin{align*}
\sum_i n_i * i = M \\
\sum_i n_i = N
\end{align*}

Phrased differently(using $p_i = n_i/N$), this says 
\begin{align*}
\sum_i p_i*i = \frac{M}{N} \\
\sum_i p_i = 1
\end{align*}

We now apply \impo{Lagrange multipliers} \question{how??}:
\begin{align*}
L = \sum_i p_i * ln(1/p_i) - \lambda[\sum_i p_i*i - M/N] - \mu [\sum_i p_i -1]
\end{align*}

from which we get
\begin{align*}
\frac{\partial_L}{\partial_{p_i}} = -[1+\ln(p_i)] - \lambda i - \mu = 0
\end{align*}

We can solve this for $p_i$:
\begin{align*}
ln(p_i) = -\lambda *i + (1+\mu) \\
p_i = e^{-\lambda_0} e^{-\lambda i}
\end{align*}

where we set $1+\mu \equiv \lambda_0$.

\impo{Putting in constraints}, we have
\begin{align*}
1 & = \sum_i p_i \\
& = \sum_i e^{-\lambda_0} e^{-\lambda i} \\
& = e^{-\lambda_0} \sum^M_{i=0} e^{-\lambda i}
\end{align*}

and 
\begin{align*}
\frac{M}{N} & = \sum_i p_i \\
& = \sum_i e^{-\lambda_0}e^{-\lambda i} * i \\
& = e^{-\lambda_0} \sum^M_{i=0}e^{\lambda i} *i
\end{align*}

We can approximate(for large $M$)
\begin{align*}
\sum^M_{i=0} e^{\lambda i} \approx \int_0^M e^{\lambda x} dx \approx \frac{1}{\lambda} \\
\sum^M_{i=1} e^{\lambda i} *i \approx \int_0^M x e^{\lambda x} dx = \frac{1}{\lambda^2}
\end{align*}

From these, we have(approximately)
\begin{align*}
e^{\lambda_0} = \frac{1}{\lambda} \\
e^{\lambda_0} \frac{M}{N} = \frac{1}{\lambda^2}
\end{align*}

From this, we get
\begin{align*}
\lambda = \frac{N}{M} = e^{\lambda_0}
\end{align*}

and thus(letting $T=\frac{M}{N}$) we have:
\begin{align*}
p_i = e^{-\lambda_0}e^{-\lambda i} = \frac{1}{T} e^{-\frac{i}{T}}
\end{align*}

This is a \impo{Boltzmann-Gibbs} distribution, where we can think of $T$(the average amount of money per agent) as the "temperature", and thus we have a "Boltzmann economy"...

Note: this distribution also solves the functional equation \question{why???}
\begin{align*}
p(m_1) p(m_2) = p(m_1 + m_2)
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application: A Power Law}
\paragraph{Model} Suppose that a (simple) economy is made up of many agents $a$, each with wealth at time $t$ in the amount of $w(a,t)$. We are interested in looking at the distribution of wealth in the economy, so we will assume there is some collection $\{w_i\}$ of possible values for the wealth an agent can have, and associated probabilities $\{p_i\}$ that an agent has wealth $\{w_i\}$. We are hoping to develop a model of the collection $\{w_i\}$.

\paragraph{Analysis} In order to apply the \impo{maximum entropy principle}, we want to look at global(aggregate/macro) observables of the system that reflect(or are made up of) characteristics of(micro) elements of the system. 

For example, we can look at the growth rate of the economy. A reasonable way to think about this is to let $R_i = w_i(t_1)/w_i(t_0)$ and $R=W(t_1)/W(t_0)$(where $t_0$ and $t_1$ represent time steps of the economy). The growth rate will then be $\ln(R)$. We then have the \impo{two constraints} on the $p_i$ \question{how the first constraint???}
\begin{align*}
\sum_i p_i *ln(R_i) = ln(R) \\
\sum_i p_i =1
\end{align*}

We now apply \impo{Lagrange multipliers}
\begin{align*}
L = \sum_i p_i \ln(1/p_i) - \lambda[\sum_i p_iln(R_i) - ln(R)] - \mu[\sum_i p_i -1]
\end{align*}

from which we get
\begin{align*}
\frac{\partial L}{\partial p_i} = -[1+\ln(p_i)] - \lambda ln(R_i) - \mu = 0
\end{align*}

We cab solve this for $p_i$
\begin{align*}
p_i = e^{-\lambda_0} e^{-\lambda ln(R_i)} = e^{-\lambda_0} (R_i)^{-\lambda}
\end{align*}
(where we have set $1+\mu \equiv \lambda_0$.)

Solving, we get $\lambda_0 = ln(Z(\lambda))$ (\impo{by using $\sum_i p_i =1$}), where $Z(\lambda) \equiv \sum_i (R_i)^{-\lambda}$(the partition function) normalizes the probability distribution to sum to 1. from this we see the power law(for $\lambda >1$):
\begin{align*}
p_i = \frac{(R_i)^{-\lambda}}{Z(\lambda)}
\end{align*}

\paragraph{Continuous Version} We let $R= w(T)/w(0)$ be the relative wealth at time $T$. We ant to find the probability density function $f(R)$, that is \impo{i.e., find f(R) that maximize the entropy}
\begin{align*}
max_{\{f\}} H(f) = - \int_1^\infty f(R) ln(f(R)) dR
\end{align*}

subject to
\begin{align*}
\int_1^\infty f(R)dR = 1 \\
\int_1^\infty f(R) ln(R) dR = Cln(R)
\end{align*}

where $C$ is the average number of transactions per time step. (\question{why????})

We need to apply the calculus of variations to maximize over a class of functions.

When we are solving an extremal problem of the form
\begin{align*}
F[x,f(x), f'(x)] dx
\end{align*}

we work to solve
\begin{align*}
\frac{\partial F}{\partial f(x)} - \frac{d}{dx}(\frac{\partial F}{\partial f'(x)}) = 0
\end{align*}

Our Lagrangian is of the form
\begin{align*}
L \equiv -\int_1^\infty f(R) ln(f(R)) dr - \mu(\int_1^\infty f(R)dR -1) - \lambda(\int_1^\infty f(R)ln(R)dR - C*ln(R))
\end{align*}

\impo{neglect following analysis...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to Physics(laser)}

\paragraph{Model} For a laser, we will be interested in the intensity of the light emitted, and the coherence property of the light will be observed in the \impo{second moment} of the intensity. The electric field \impo{strength} of such a laser will have the form
\begin{align*}
E(x,t) = E(t)sin(kx)
\end{align*}

and $E(t)$ can be decomposed in the form 
\begin{align*}
E(t) = B*e^{-iwt} + B*e^{iwt}
\end{align*}

If we measure the intensity of the light over time intervals long compared to the frequency, but small compared to fluctuations of $B(t)$, the output will be proportional to $BB*$ and to the \impo{loss rate} $sk$, of the laser:
\begin{align*}
I = 2kBB*
\end{align*}
The \impo{intensity} squared will be
\begin{align*}
I^2 = 4k^2B^2B*^2
\end{align*}

\paragraph{Analysis}
If we assume that $B$ and $B^*$ are continuous random variables associated with a stationary process, then the \impo{information entropy} of the system will be
\begin{align*}
H = \int p(B,B*) log(\frac{1}{p(B,B*)}) d^2B
\end{align*}

The \impo{two constraints} \question{why???} on the system will be the averages of the intensity and the square of the intensity:
\begin{align*}
f_1 = <2kBB*>,\\
f_2 = <4k^2B^2B*^2>
\end{align*}

Then, of course, we will let
\begin{align*}
f^{(1)}_{B,B*} = 2kBB* \\
f^{(2)}_{B,B*} = 4k^2B^2B*^2
\end{align*}


We can now use the method outlined above, finding the maximum entropy general solution derived via Lagrange multipliers for this system.

Applying the general solution, we get 
\begin{align*}
P(B,B^*) = exp[-\lambda - \lambda_1 2kBB* - \lambda_2 4k^2 B^2 b*^2]
\end{align*}
or in other notation:
\begin{align*}
p(B,B^*) = N*exp(-\alpha |B|^2 - \beta |B|^4)
\end{align*}

This function in laser physics is typically derived by solving the \impo{Fokker-Planck} equation belonging to the \impo{Langevin equation} for the system. 



\end{document}